{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "310f2be1",
   "metadata": {},
   "source": [
    "### Middeleware\n",
    "##### Middleware provides a way to more tightly control what happens inside the agent. Middleware is useful for the following:\n",
    "* Tracking agent behavior with logging,analytics, and debugging\n",
    "* Transforming prompts,tool selection, and output formatting\n",
    "* Adding retries, fallbacks, and early termination logic\n",
    "* Applying rate limits, guardrails and PII detection\n",
    "##### it can be model check, exception checking, etc., etc.\n",
    "##### Agent with Middleware --> we can add some hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a515607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71d18f56",
   "metadata": {},
   "source": [
    "### Built-in Middleware (Ref: https://docs.langchain.com/oss/python/langchain/middleware/built-in#built-in-middleware)\n",
    "* 1. Summarization ===> we add this middleware, after some messages it will summarize. This summarization of middleware can be taken care by LLM\n",
    "* 2. Human-in-the-loop ===> \n",
    "* 3. Model call limit\n",
    "* 4. Tool call limit \n",
    "* 5. PII detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5997e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import keys \n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d660a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f66acc1",
   "metadata": {},
   "source": [
    "### Summarization Middleware\n",
    "##### Automatically summarize conversation history when approaching token limits, preserving recent messages while compressing older context. Summarization is useful for the following:\n",
    "* Long-running conversations that exceed context windows.\n",
    "* Multi-turn dialogues with extensive history.\n",
    "* Applications where preserving full conversation context matters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d668604",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1de7f966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[], # empty as not defined yet\n",
    "    checkpointer=InMemorySaver(),\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(     # Applying Summarization within our agent\n",
    "            model=\"gpt-4o-mini\",     # What LLM model to use for summarization? Here, gpt-4o-mini\n",
    "            trigger=(\"tokens\",10),   # when messages length become 10\n",
    "            keep=(\"messages\",4)     # keep the top 4 messages while preserving\n",
    "                                    # add any number of middleware techniques as required \n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25643549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before invoke this agent, create a thread \n",
    "# Run with thread id \n",
    "config={\"configurable\":{\"thread_id\":\"test-1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "085658d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages:{'messages': [HumanMessage(content='Here is a summary of the conversation to date:\\n\\nWhat is 2+2? -> 2 + 2 equals 4.  \\nWhat is 2-2? -> 2 - 2 equals 0.  \\nWhat is 2*2? -> 2 * 2 equals 4.  \\nWhat is 20+2? -> 20 + 2 equals 22.  \\nWhat is 22+22? -> 22 + 22 equals 44.  \\nWhat is 200+2? -> 200 + 2 equals 202.  \\nWhat is 2000*2? -> 2000 * 2 equals 4000.', additional_kwargs={}, response_metadata={}, id='56cf36cd-f806-4e05-9986-71bb7217dabc'), AIMessage(content='200 + 2 equals 202.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 191, 'total_tokens': 199, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-Crajig6MH5E10HyZaOxGe1HlCmjgT', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c4-f8cd-7321-b46c-3b79171f8827-0', usage_metadata={'input_tokens': 191, 'output_tokens': 8, 'total_tokens': 199, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What is 2000*2?What is 500/0?What is 100+200-300*500?', additional_kwargs={}, response_metadata={}, id='c25fa05c-2bcb-4a3e-8765-529cf8e4b94f'), AIMessage(content='- \\\\(2000 \\\\times 2\\\\) equals \\\\(4000\\\\).\\n- \\\\(500 \\\\div 0\\\\) is undefined, as division by zero is not possible in mathematics.\\n- \\\\(100 + 200 - 300 \\\\times 500\\\\) equals \\\\(100 + 200 - 150000\\\\), which simplifies to \\\\(300 - 150000 = -149700\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 80, 'prompt_tokens': 210, 'total_tokens': 290, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-Crajl9tYT4X2Qw6GB5nCsrnBwUiiH', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c5-0609-75a1-9685-55b694da7ae8-0', usage_metadata={'input_tokens': 210, 'output_tokens': 80, 'total_tokens': 290, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What is 3+5?', additional_kwargs={}, response_metadata={}, id='79ec67a3-7bcc-48f5-8c43-0d2ddb122986'), AIMessage(content='\\\\(3 + 5\\\\) equals \\\\(8\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 282, 'total_tokens': 294, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-CralWvkKTPwk1BzbAVyE4YkvoXV0J', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c6-ae33-7800-8d22-fa4b5145e076-0', usage_metadata={'input_tokens': 282, 'output_tokens': 12, 'total_tokens': 294, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "Messages:6\n",
      "Messages:{'messages': [HumanMessage(content='Here is a summary of the conversation to date:\\n\\nWhat is 2+2? -> 2 + 2 equals 4.  \\nWhat is 2-2? -> 2 - 2 equals 0.  \\nWhat is 2*2? -> 2 * 2 equals 4.  \\nWhat is 20+2? -> 20 + 2 equals 22.  \\nWhat is 22+22? -> 22 + 22 equals 44.  \\nWhat is 200+2? -> 200 + 2 equals 202.  \\nWhat is 2000*2? -> 2000 * 2 equals 4000.  \\nWhat is 500/0? What is 100+200-300*500?', additional_kwargs={}, response_metadata={}, id='9c6d1357-f8b2-437e-9041-f1651252d643'), AIMessage(content='- \\\\(2000 \\\\times 2\\\\) equals \\\\(4000\\\\).\\n- \\\\(500 \\\\div 0\\\\) is undefined, as division by zero is not possible in mathematics.\\n- \\\\(100 + 200 - 300 \\\\times 500\\\\) equals \\\\(100 + 200 - 150000\\\\), which simplifies to \\\\(300 - 150000 = -149700\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 80, 'prompt_tokens': 210, 'total_tokens': 290, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-Crajl9tYT4X2Qw6GB5nCsrnBwUiiH', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c5-0609-75a1-9685-55b694da7ae8-0', usage_metadata={'input_tokens': 210, 'output_tokens': 80, 'total_tokens': 290, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What is 3+5?', additional_kwargs={}, response_metadata={}, id='79ec67a3-7bcc-48f5-8c43-0d2ddb122986'), AIMessage(content='\\\\(3 + 5\\\\) equals \\\\(8\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 282, 'total_tokens': 294, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-CralWvkKTPwk1BzbAVyE4YkvoXV0J', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c6-ae33-7800-8d22-fa4b5145e076-0', usage_metadata={'input_tokens': 282, 'output_tokens': 12, 'total_tokens': 294, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What is 10-4?', additional_kwargs={}, response_metadata={}, id='55063cb0-f943-4537-afbc-68f9458b7954'), AIMessage(content='\\\\(10 - 4\\\\) equals \\\\(6\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 286, 'total_tokens': 298, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-CralaSFRPepXRwdJuC3OZ8z5Q6y69', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c6-be17-7de1-aaf5-17adb6c40410-0', usage_metadata={'input_tokens': 286, 'output_tokens': 12, 'total_tokens': 298, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "Messages:6\n",
      "Messages:{'messages': [HumanMessage(content='Here is a summary of the conversation to date:\\n\\nWhat is 2+2? -> 2 + 2 equals 4.  \\nWhat is 2-2? -> 2 - 2 equals 0.  \\nWhat is 2*2? -> 2 * 2 equals 4.  \\nWhat is 20+2? -> 20 + 2 equals 22.  \\nWhat is 22+22? -> 22 + 22 equals 44.  \\nWhat is 200+2? -> 200 + 2 equals 202.  \\nWhat is 2000*2? -> 2000 * 2 equals 4000.  \\nWhat is 500/0? -> Division by zero is undefined.  \\nWhat is 100+200-300*500? -> 100 + 200 - 300 * 500 = -149700.  \\nWhat is 3+5?', additional_kwargs={}, response_metadata={}, id='05e431a7-27be-4820-9552-dd8b6c038643'), AIMessage(content='\\\\(3 + 5\\\\) equals \\\\(8\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 282, 'total_tokens': 294, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-CralWvkKTPwk1BzbAVyE4YkvoXV0J', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c6-ae33-7800-8d22-fa4b5145e076-0', usage_metadata={'input_tokens': 282, 'output_tokens': 12, 'total_tokens': 294, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What is 10-4?', additional_kwargs={}, response_metadata={}, id='55063cb0-f943-4537-afbc-68f9458b7954'), AIMessage(content='\\\\(10 - 4\\\\) equals \\\\(6\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 286, 'total_tokens': 298, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-CralaSFRPepXRwdJuC3OZ8z5Q6y69', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c6-be17-7de1-aaf5-17adb6c40410-0', usage_metadata={'input_tokens': 286, 'output_tokens': 12, 'total_tokens': 298, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What is 6*7?', additional_kwargs={}, response_metadata={}, id='3cf8041b-0b6c-433e-8e2b-c79af1fee7d0'), AIMessage(content='\\\\(6 \\\\times 7\\\\) equals \\\\(42\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 251, 'total_tokens': 264, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-CralfZjQ6zme2EaUvRICxpQv6Ot8L', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c6-d062-73a2-8bd1-17b0fa88760a-0', usage_metadata={'input_tokens': 251, 'output_tokens': 13, 'total_tokens': 264, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "Messages:6\n",
      "Messages:{'messages': [HumanMessage(content='Here is a summary of the conversation to date:\\n\\nThe conversation involves a series of mathematical questions and answers. Key exchanges include:\\n\\n- \\\\(2 + 2\\\\) equals \\\\(4\\\\)\\n- \\\\(2 - 2\\\\) equals \\\\(0\\\\)\\n- \\\\(2 \\\\times 2\\\\) equals \\\\(4\\\\)\\n- \\\\(20 + 2\\\\) equals \\\\(22\\\\)\\n- \\\\(22 + 22\\\\) equals \\\\(44\\\\)\\n- \\\\(200 + 2\\\\) equals \\\\(202\\\\)\\n- \\\\(2000 \\\\times 2\\\\) equals \\\\(4000\\\\)\\n- Division by zero is undefined for \\\\(500/0\\\\)\\n- \\\\(100 + 200 - 300 \\\\times 500\\\\) equals \\\\(-149700\\\\)\\n- \\\\(3 + 5\\\\) equals \\\\(8\\\\)\\n- \\\\(10 - 4\\\\) is the next question posed.', additional_kwargs={}, response_metadata={}, id='fec08c2a-dc71-479a-a6be-93f4e432c8c0'), AIMessage(content='\\\\(10 - 4\\\\) equals \\\\(6\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 286, 'total_tokens': 298, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-CralaSFRPepXRwdJuC3OZ8z5Q6y69', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c6-be17-7de1-aaf5-17adb6c40410-0', usage_metadata={'input_tokens': 286, 'output_tokens': 12, 'total_tokens': 298, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What is 6*7?', additional_kwargs={}, response_metadata={}, id='3cf8041b-0b6c-433e-8e2b-c79af1fee7d0'), AIMessage(content='\\\\(6 \\\\times 7\\\\) equals \\\\(42\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 251, 'total_tokens': 264, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-CralfZjQ6zme2EaUvRICxpQv6Ot8L', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c6-d062-73a2-8bd1-17b0fa88760a-0', usage_metadata={'input_tokens': 251, 'output_tokens': 13, 'total_tokens': 264, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What is 18+9?', additional_kwargs={}, response_metadata={}, id='161310c3-1b11-4234-865a-30822ed10290'), AIMessage(content='\\\\(18 + 9\\\\) equals \\\\(27\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 244, 'total_tokens': 256, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-CralkssWSAAwraSRKXd4GT0EYH9Eb', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c6-e3ca-7e52-975a-4d9828c22f06-0', usage_metadata={'input_tokens': 244, 'output_tokens': 12, 'total_tokens': 256, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "Messages:6\n",
      "Messages:{'messages': [HumanMessage(content='Here is a summary of the conversation to date:\\n\\nThe conversation involves a series of mathematical questions and answers, including the following key exchanges:\\n\\n- \\\\(2 + 2\\\\) equals \\\\(4\\\\)\\n- \\\\(2 - 2\\\\) equals \\\\(0\\\\)\\n- \\\\(2 \\\\times 2\\\\) equals \\\\(4\\\\)\\n- \\\\(20 + 2\\\\) equals \\\\(22\\\\)\\n- \\\\(22 + 22\\\\) equals \\\\(44\\\\)\\n- \\\\(200 + 2\\\\) equals \\\\(202\\\\)\\n- \\\\(2000 \\\\times 2\\\\) equals \\\\(4000\\\\)\\n- Division by zero is undefined for \\\\(500/0\\\\)\\n- \\\\(100 + 200 - 300 \\\\times 500\\\\) equals \\\\(-149700\\\\)\\n- \\\\(3 + 5\\\\) equals \\\\(8\\\\)\\n- \\\\(10 - 4\\\\) equals \\\\(6\\\\)\\n- The next question posed is \\\\(6 \\\\times 7\\\\).', additional_kwargs={}, response_metadata={}, id='46b76c8e-28ad-4ba0-9f93-a9da94e74ac0'), AIMessage(content='\\\\(6 \\\\times 7\\\\) equals \\\\(42\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 251, 'total_tokens': 264, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-CralfZjQ6zme2EaUvRICxpQv6Ot8L', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c6-d062-73a2-8bd1-17b0fa88760a-0', usage_metadata={'input_tokens': 251, 'output_tokens': 13, 'total_tokens': 264, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What is 18+9?', additional_kwargs={}, response_metadata={}, id='161310c3-1b11-4234-865a-30822ed10290'), AIMessage(content='\\\\(18 + 9\\\\) equals \\\\(27\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 244, 'total_tokens': 256, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-CralkssWSAAwraSRKXd4GT0EYH9Eb', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c6-e3ca-7e52-975a-4d9828c22f06-0', usage_metadata={'input_tokens': 244, 'output_tokens': 12, 'total_tokens': 256, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What is 45-17?', additional_kwargs={}, response_metadata={}, id='643e9313-d4b8-4451-a232-aa72055bd23c'), AIMessage(content='\\\\(45 - 17\\\\) equals \\\\(28\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 259, 'total_tokens': 271, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bbc38b4db', 'id': 'chatcmpl-Cralp8Xf23wgN0zy8jk7EFpSFBYXa', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c6-f6f8-7c63-9d66-0f1bb31f5190-0', usage_metadata={'input_tokens': 259, 'output_tokens': 12, 'total_tokens': 271, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "Messages:6\n",
      "Messages:{'messages': [HumanMessage(content='Here is a summary of the conversation to date:\\n\\nThe conversation involves a series of mathematical questions and answers, including key exchanges such as:\\n\\n- \\\\(2 + 2\\\\) equals \\\\(4\\\\)\\n- \\\\(2 - 2\\\\) equals \\\\(0\\\\)\\n- \\\\(2 \\\\times 2\\\\) equals \\\\(4\\\\)\\n- \\\\(20 + 2\\\\) equals \\\\(22\\\\)\\n- \\\\(22 + 22\\\\) equals \\\\(44\\\\)\\n- \\\\(200 + 2\\\\) equals \\\\(202\\\\)\\n- \\\\(2000 \\\\times 2\\\\) equals \\\\(4000\\\\)\\n- Division by zero is undefined for \\\\(500/0\\\\)\\n- \\\\(100 + 200 - 300 \\\\times 500\\\\) equals \\\\(-149700\\\\)\\n- \\\\(3 + 5\\\\) equals \\\\(8\\\\)\\n- \\\\(10 - 4\\\\) equals \\\\(6\\\\)\\n- The last question posed was \\\\(6 \\\\times 7\\\\) which equals \\\\(42\\\\).\\n- A new question asked is \\\\(18 + 9\\\\).', additional_kwargs={}, response_metadata={}, id='4872fde2-28ad-4137-9424-7ced90fb0ce6'), AIMessage(content='\\\\(18 + 9\\\\) equals \\\\(27\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 244, 'total_tokens': 256, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-CralkssWSAAwraSRKXd4GT0EYH9Eb', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c6-e3ca-7e52-975a-4d9828c22f06-0', usage_metadata={'input_tokens': 244, 'output_tokens': 12, 'total_tokens': 256, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What is 45-17?', additional_kwargs={}, response_metadata={}, id='643e9313-d4b8-4451-a232-aa72055bd23c'), AIMessage(content='\\\\(45 - 17\\\\) equals \\\\(28\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 259, 'total_tokens': 271, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bbc38b4db', 'id': 'chatcmpl-Cralp8Xf23wgN0zy8jk7EFpSFBYXa', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c6-f6f8-7c63-9d66-0f1bb31f5190-0', usage_metadata={'input_tokens': 259, 'output_tokens': 12, 'total_tokens': 271, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What is 8*9?', additional_kwargs={}, response_metadata={}, id='ee7187e8-8e0b-48a5-8584-daf167f3ccb2'), AIMessage(content='\\\\(8 \\\\times 9\\\\) equals \\\\(72\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 277, 'total_tokens': 290, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-CralvTFrmAUrLUTOCabwxDe0mCKif', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c7-0f85-7b42-9efb-7e0b96823981-0', usage_metadata={'input_tokens': 277, 'output_tokens': 13, 'total_tokens': 290, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "Messages:6\n",
      "Messages:{'messages': [HumanMessage(content='Here is a summary of the conversation to date:\\n\\nThe conversation involves a series of mathematical questions and answers, including key exchanges such as:\\n\\n- \\\\\\\\(2 + 2\\\\\\\\) equals \\\\\\\\(4\\\\\\\\)\\n- \\\\\\\\(2 - 2\\\\\\\\) equals \\\\\\\\(0\\\\\\\\)\\n- \\\\\\\\(2 \\\\\\\\times 2\\\\\\\\) equals \\\\\\\\(4\\\\\\\\)\\n- \\\\\\\\(20 + 2\\\\\\\\) equals \\\\\\\\(22\\\\\\\\)\\n- \\\\\\\\(22 + 22\\\\\\\\) equals \\\\\\\\(44\\\\\\\\)\\n- \\\\\\\\(200 + 2\\\\\\\\) equals \\\\\\\\(202\\\\\\\\)\\n- \\\\\\\\(2000 \\\\\\\\times 2\\\\\\\\) equals \\\\\\\\(4000\\\\\\\\)\\n- Division by zero is undefined for \\\\\\\\(500/0\\\\\\\\)\\n- \\\\\\\\(100 + 200 - 300 \\\\\\\\times 500\\\\\\\\) equals \\\\\\\\(-149700\\\\\\\\)\\n- \\\\\\\\(3 + 5\\\\\\\\) equals \\\\\\\\(8\\\\\\\\)\\n- \\\\\\\\(10 - 4\\\\\\\\) equals \\\\\\\\(6\\\\\\\\)\\n- The last question posed was \\\\\\\\(6 \\\\\\\\times 7\\\\\\\\) which equals \\\\\\\\(42\\\\\\\\).\\n- A new question asked is \\\\\\\\(18 + 9\\\\\\\\) which equals \\\\\\\\(27\\\\\\\\).\\n- The latest question is \\\\\\\\(45 - 17\\\\\\\\).', additional_kwargs={}, response_metadata={}, id='21933aa8-3466-40e3-8d84-ef76adcc55ef'), AIMessage(content='\\\\(45 - 17\\\\) equals \\\\(28\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 259, 'total_tokens': 271, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bbc38b4db', 'id': 'chatcmpl-Cralp8Xf23wgN0zy8jk7EFpSFBYXa', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c6-f6f8-7c63-9d66-0f1bb31f5190-0', usage_metadata={'input_tokens': 259, 'output_tokens': 12, 'total_tokens': 271, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What is 8*9?', additional_kwargs={}, response_metadata={}, id='ee7187e8-8e0b-48a5-8584-daf167f3ccb2'), AIMessage(content='\\\\(8 \\\\times 9\\\\) equals \\\\(72\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 277, 'total_tokens': 290, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-CralvTFrmAUrLUTOCabwxDe0mCKif', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c7-0f85-7b42-9efb-7e0b96823981-0', usage_metadata={'input_tokens': 277, 'output_tokens': 13, 'total_tokens': 290, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What is 100+25?', additional_kwargs={}, response_metadata={}, id='3a913945-e330-4d9a-8bfa-2da183448e8a'), AIMessage(content='\\\\(100 + 25\\\\) equals \\\\(125\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 321, 'total_tokens': 333, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-Cram1JBaa9La3SxtK64LlxggIBIqk', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c7-293c-7992-aa07-a62975cd99fc-0', usage_metadata={'input_tokens': 321, 'output_tokens': 12, 'total_tokens': 333, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "Messages:6\n",
      "Messages:{'messages': [HumanMessage(content='Here is a summary of the conversation to date:\\n\\nThe conversation involves a series of mathematical questions and answers, including key exchanges such as:\\n\\n- \\\\(2 + 2\\\\) equals \\\\(4\\\\)\\n- \\\\(2 - 2\\\\) equals \\\\(0\\\\)\\n- \\\\(2 \\\\times 2\\\\) equals \\\\(4\\\\)\\n- \\\\(20 + 2\\\\) equals \\\\(22\\\\)\\n- \\\\(22 + 22\\\\) equals \\\\(44\\\\)\\n- \\\\(200 + 2\\\\) equals \\\\(202\\\\)\\n- \\\\(2000 \\\\times 2\\\\) equals \\\\(4000\\\\)\\n- Division by zero is undefined for \\\\(500/0\\\\)\\n- \\\\(100 + 200 - 300 \\\\times 500\\\\) equals \\\\(-149700\\\\)\\n- \\\\(3 + 5\\\\) equals \\\\(8\\\\)\\n- \\\\(10 - 4\\\\) equals \\\\(6\\\\)\\n- Last answered question was \\\\(6 \\\\times 7\\\\) which equals \\\\(42\\\\).\\n- New question posed is \\\\(18 + 9\\\\) which equals \\\\(27\\\\).\\n- Latest question is \\\\(45 - 17\\\\) which equals \\\\(28\\\\).\\n- A new question asked is \\\\(8 \\\\times 9\\\\).', additional_kwargs={}, response_metadata={}, id='6bf048df-59c3-49f2-98fb-019e97d2daf6'), AIMessage(content='\\\\(8 \\\\times 9\\\\) equals \\\\(72\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 277, 'total_tokens': 290, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-CralvTFrmAUrLUTOCabwxDe0mCKif', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c7-0f85-7b42-9efb-7e0b96823981-0', usage_metadata={'input_tokens': 277, 'output_tokens': 13, 'total_tokens': 290, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What is 100+25?', additional_kwargs={}, response_metadata={}, id='3a913945-e330-4d9a-8bfa-2da183448e8a'), AIMessage(content='\\\\(100 + 25\\\\) equals \\\\(125\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 321, 'total_tokens': 333, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-Cram1JBaa9La3SxtK64LlxggIBIqk', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c7-293c-7992-aa07-a62975cd99fc-0', usage_metadata={'input_tokens': 321, 'output_tokens': 12, 'total_tokens': 333, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What is 64/8?', additional_kwargs={}, response_metadata={}, id='c57d493b-8d74-4c28-ae0b-ceb1795946dc'), AIMessage(content='\\\\(64 \\\\div 8\\\\) equals \\\\(8\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 313, 'total_tokens': 326, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-Cram8CszlJBd7aSkXYg3PqvvIAOlD', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c7-4338-7152-ac31-df84364e0fbc-0', usage_metadata={'input_tokens': 313, 'output_tokens': 13, 'total_tokens': 326, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "Messages:6\n",
      "Messages:{'messages': [HumanMessage(content='Here is a summary of the conversation to date:\\n\\nThe conversation involves a series of mathematical questions and answers, including key exchanges such as:\\n\\n- \\\\(2 + 2\\\\) equals \\\\(4\\\\)\\n- \\\\(2 - 2\\\\) equals \\\\(0\\\\)\\n- \\\\(2 \\\\times 2\\\\) equals \\\\(4\\\\)\\n- \\\\(20 + 2\\\\) equals \\\\(22\\\\)\\n- \\\\(22 + 22\\\\) equals \\\\(44\\\\)\\n- \\\\(200 + 2\\\\) equals \\\\(202\\\\)\\n- \\\\(2000 \\\\times 2\\\\) equals \\\\(4000\\\\)\\n- Division by zero, as in \\\\(500/0\\\\), is undefined.\\n- \\\\(100 + 200 - 300 \\\\times 500\\\\) equals \\\\(-149700\\\\)\\n- \\\\(3 + 5\\\\) equals \\\\(8\\\\)\\n- \\\\(10 - 4\\\\) equals \\\\(6\\\\)\\n- The last answered question was \\\\(6 \\\\times 7\\\\) which equals \\\\(42\\\\).\\n- A new question posed is \\\\(18 + 9\\\\) which equals \\\\(27\\\\).\\n- The latest question is \\\\(45 - 17\\\\) which equals \\\\(28\\\\).\\n- A new question asked is \\\\(8 \\\\times 9\\\\) which equals \\\\(72\\\\).\\n- Another question is posed: What is \\\\(100 + 25\\\\)?', additional_kwargs={}, response_metadata={}, id='6e51f864-5fed-4ccf-9492-d996d500035b'), AIMessage(content='\\\\(100 + 25\\\\) equals \\\\(125\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 321, 'total_tokens': 333, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-Cram1JBaa9La3SxtK64LlxggIBIqk', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c7-293c-7992-aa07-a62975cd99fc-0', usage_metadata={'input_tokens': 321, 'output_tokens': 12, 'total_tokens': 333, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What is 64/8?', additional_kwargs={}, response_metadata={}, id='c57d493b-8d74-4c28-ae0b-ceb1795946dc'), AIMessage(content='\\\\(64 \\\\div 8\\\\) equals \\\\(8\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 313, 'total_tokens': 326, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-Cram8CszlJBd7aSkXYg3PqvvIAOlD', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c7-4338-7152-ac31-df84364e0fbc-0', usage_metadata={'input_tokens': 313, 'output_tokens': 13, 'total_tokens': 326, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What is 12*4?', additional_kwargs={}, response_metadata={}, id='1d5cc303-2419-4bf7-8ad8-04d25da42692'), AIMessage(content='\\\\(12 \\\\times 4\\\\) equals \\\\(48\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 340, 'total_tokens': 353, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-CramE3bhCWhfzo8vc0rELM1ypDryL', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c7-5b5f-7b63-bafd-cad1980f8969-0', usage_metadata={'input_tokens': 340, 'output_tokens': 13, 'total_tokens': 353, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "Messages:6\n",
      "Messages:{'messages': [HumanMessage(content='Here is a summary of the conversation to date:\\n\\nThe conversation includes a series of mathematical questions and answers, such as:\\n- \\\\(2 + 2 = 4\\\\)\\n- \\\\(2 - 2 = 0\\\\)\\n- \\\\(2 \\\\times 2 = 4\\\\)\\n- \\\\(20 + 2 = 22\\\\)\\n- \\\\(22 + 22 = 44\\\\)\\n- \\\\(200 + 2 = 202\\\\)\\n- \\\\(2000 \\\\times 2 = 4000\\\\)\\n- Division by zero: \\\\(500/0\\\\) is undefined.\\n- \\\\(100 + 200 - 300 \\\\times 500 = -149700\\\\)\\n- \\\\(3 + 5 = 8\\\\)\\n- \\\\(10 - 4 = 6\\\\)\\n- The last answered question was \\\\(6 \\\\times 7 = 42\\\\).\\n- New questions include:\\n  - \\\\(18 + 9 = 27\\\\)\\n  - \\\\(45 - 17 = 28\\\\)\\n  - \\\\(8 \\\\times 9 = 72\\\\)\\n  - \\\\(100 + 25 = 125\\\\) \\n- A new question asked is \\\\(64/8\\\\).', additional_kwargs={}, response_metadata={}, id='d25f5318-a125-4e24-9849-d3eaa225251f'), AIMessage(content='\\\\(64 \\\\div 8\\\\) equals \\\\(8\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 313, 'total_tokens': 326, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-Cram8CszlJBd7aSkXYg3PqvvIAOlD', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c7-4338-7152-ac31-df84364e0fbc-0', usage_metadata={'input_tokens': 313, 'output_tokens': 13, 'total_tokens': 326, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What is 12*4?', additional_kwargs={}, response_metadata={}, id='1d5cc303-2419-4bf7-8ad8-04d25da42692'), AIMessage(content='\\\\(12 \\\\times 4\\\\) equals \\\\(48\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 340, 'total_tokens': 353, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-CramE3bhCWhfzo8vc0rELM1ypDryL', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c7-5b5f-7b63-bafd-cad1980f8969-0', usage_metadata={'input_tokens': 340, 'output_tokens': 13, 'total_tokens': 353, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What is 90-36?', additional_kwargs={}, response_metadata={}, id='38a9d778-e9a6-4006-9f82-c5223191a94b'), AIMessage(content='\\\\(90 - 36\\\\) equals \\\\(54\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 307, 'total_tokens': 319, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-CramKN8b6n72Fs0uycrfdGXZEgQv2', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62c7-73aa-7ed1-bdcd-5c080b4a6760-0', usage_metadata={'input_tokens': 307, 'output_tokens': 12, 'total_tokens': 319, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "Messages:6\n"
     ]
    }
   ],
   "source": [
    "# Alternative test data \n",
    "questions=[\n",
    "    \"What is 3+5?\",\n",
    "    \"What is 10-4?\",\n",
    "    \"What is 6*7?\",\n",
    "    \"What is 18+9?\",\n",
    "    \"What is 45-17?\",\n",
    "    \"What is 8*9?\",\n",
    "    \"What is 100+25?\",\n",
    "    \"What is 64/8?\",\n",
    "    \"What is 12*4?\",\n",
    "    \"What is 90-36?\"\n",
    "]\n",
    "\n",
    "\n",
    "for q in questions:\n",
    "    response=agent.invoke({\"messages\":[HumanMessage(content=q)]},config)\n",
    "    print(f\"Messages:{response}\")\n",
    "    print(f\"Messages:{len(response['messages'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75594000",
   "metadata": {},
   "source": [
    "### token size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b841ffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def search_hotels(city:str)->str:\n",
    "    \"\"\"search tokens - returns long response to use more tokens\"\"\"\n",
    "    return f\"\"\"Hotels in {city}:\n",
    "    1. Grand Hotel - 5 star, $350/night, spa, pool, gym\n",
    "    2. City Inn - 4 star, $180/night, business center \n",
    "    3. Budget Stay - 3 star, $75/night, free wifi\"\"\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[search_hotels], # empty as not defined yet\n",
    "    checkpointer=InMemorySaver(),\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(      # Applying Summarization within our agent\n",
    "            model=\"gpt-4o-mini\",      # What LLM model to use for summarization? Here, gpt-4o-mini\n",
    "            trigger=(\"tokens\",500),   # when messages length become 10\n",
    "            keep=(\"tokens\",200)      # keep the top 4 messages while preserving\n",
    "                                     # add any number of middleware techniques as required \n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#Create Config\n",
    "config={\"configurable\":{\"thread_id\":\"test-1\"}}\n",
    "\n",
    "# Token counter (approximate)\n",
    "def count_tokens(messages):\n",
    "    total_chars = sum(len(str(m.content)) for m in messages)\n",
    "    return total_chars // 4  # 4 chars = 1 token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d7df84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zurich:~573 tokens, 6 messages\n",
      "[HumanMessage(content='Here is a summary of the conversation to date:\\n\\nUser requested hotel information for several cities: Paris, NYC, London, Dubai, Mumbai, and Milan. The AI provided hotel lists with details on star ratings, prices, and amenities for each city:\\n\\n**Hotels in Paris:**\\n1. Grand Hotel - 5 stars, $350/night, Amenities: Spa, pool, gym\\n2. City Inn - 4 stars, $180/night, Amenities: Business center\\n3. Budget Stay - 3 stars, $75/night, Amenities: Free wifi\\n\\n**Hotels in NYC:**\\n1. Grand Hotel - 5 stars, $350/night, Amenities: Spa, pool, gym\\n2. City Inn - 4 stars, $180/night, Amenities: Business center\\n3. Budget Stay - 3 stars, $75/night, Amenities: Free wifi\\n\\n**Hotels in London:**\\n1. Grand Hotel - 5 stars, $350/night, Amenities: Spa, pool, gym\\n2. City Inn - 4 stars, $180/night, Amenities: Business center\\n3. Budget Stay - 3 stars, $75/night, Amenities: Free wifi\\n\\n**Hotels in Dubai:**\\n1. Grand Hotel - 5 stars, $350/night, Amenities: Spa, pool, gym\\n2. City Inn - 4 stars, $180/night, Amenities: Business center\\n3. Budget Stay - 3 stars, $75/night, Amenities: Free wifi\\n\\n**Hotels in Mumbai:**\\n1. Grand Hotel - 5 stars, $350/night, Amenities: Spa, pool, gym\\n2. City Inn - 4 stars, $180/night, Amenities: Business center\\n3. Budget Stay - 3 stars, $75/night, Amenities: Free wifi\\n\\n**Hotels in Milan:**\\n1. Grand Hotel - 5 stars, $350/night, Amenities: Spa, pool, gym\\n2. City Inn - 4 stars, $180/night, Amenities: Business center\\n3. Budget Stay - 3 stars, $75/night, Amenities: Free wifi', additional_kwargs={}, response_metadata={}, id='880d01a6-9814-4d76-9d34-1c8b066d6729'), AIMessage(content='Here are some hotels in Milan:\\n\\n1. **Grand Hotel**\\n   - Rating: 5 stars\\n   - Price: $350/night\\n   - Amenities: Spa, pool, gym\\n\\n2. **City Inn**\\n   - Rating: 4 stars\\n   - Price: $180/night\\n   - Amenities: Business center\\n\\n3. **Budget Stay**\\n   - Rating: 3 stars\\n   - Price: $75/night\\n   - Amenities: Free wifi', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 97, 'prompt_tokens': 605, 'total_tokens': 702, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_644f11dd4d', 'id': 'chatcmpl-Crb1Xs2tfOsMq0ZvCGLPUEZOH0lgS', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62d5-d6c7-7e03-8f2f-104cc52c2516-0', usage_metadata={'input_tokens': 605, 'output_tokens': 97, 'total_tokens': 702, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='Find the hotel in Zurich', additional_kwargs={}, response_metadata={}, id='0742da9d-da45-4fd6-818a-6329449bcf84'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 614, 'total_tokens': 630, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-Crb1grCcagRpV4glIJktAr9zqPnm2', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b62d5-fa21-7071-9337-fe5cd03945ba-0', tool_calls=[{'name': 'search_hotels', 'args': {'city': 'Zurich'}, 'id': 'call_RAr7v0ldVuv8iGWPiiaRBdzO', 'type': 'tool_call'}], usage_metadata={'input_tokens': 614, 'output_tokens': 16, 'total_tokens': 630, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Hotels in Zurich:\\n    1. Grand Hotel - 5 star, $350/night, spa, pool, gym\\n    2. City Inn - 4 star, $180/night, business center \\n    3. Budget Stay - 3 star, $75/night, free wifi', name='search_hotels', id='4b0df077-e99c-48ee-926f-ae19f61808b1', tool_call_id='call_RAr7v0ldVuv8iGWPiiaRBdzO'), AIMessage(content='Here are some hotels in Zurich:\\n\\n1. **Grand Hotel**\\n   - Rating: 5 stars\\n   - Price: $350/night\\n   - Amenities: Spa, pool, gym\\n\\n2. **City Inn**\\n   - Rating: 4 stars\\n   - Price: $180/night\\n   - Amenities: Business center\\n\\n3. **Budget Stay**\\n   - Rating: 3 stars\\n   - Price: $75/night\\n   - Amenities: Free wifi', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 97, 'prompt_tokens': 670, 'total_tokens': 767, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_644f11dd4d', 'id': 'chatcmpl-Crb1phP0K5YwW6OwmaPLNaHFy9vBe', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b62d6-1ab8-7471-a85b-46f9f0cb110f-0', usage_metadata={'input_tokens': 670, 'output_tokens': 97, 'total_tokens': 767, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n"
     ]
    }
   ],
   "source": [
    "# Run test \n",
    "cities=[\"Paris\",\"NYC\",\"London\",\"Dubai\",\"Mumbai\",\"Milan\",\"Zurich\"]\n",
    "\n",
    "for city in cities:\n",
    "    response=agent.invoke(\n",
    "        {\"messages\":[HumanMessage(content=f\"Find the hotel in {city}\")]},\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "tokens=count_tokens(response[\"messages\"])\n",
    "print(f\"{city}:~{tokens} tokens, {len(response['messages'])} messages\")\n",
    "print(f\"{(response['messages'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017c5c46",
   "metadata": {},
   "source": [
    "### Human-in-the-loop Middleware\n",
    "##### Pause agent execution for human approval, editing, or rejection of tool calls before they execute. Human-in-the-loop is useful for the following:\n",
    "* High-stakes operations requiring human approval (eg. database writes,financial transactions)\n",
    "* Compliance workflows where human oversight is mandatory\n",
    "* Long-running conversations where human feedback guides the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e57fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import HumanInTheLoopMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "def read_email_tool(email_id:str) -> str:\n",
    "    \"\"\"Sample function to read an email by its ID.\"\"\"\n",
    "    return f\"Email content for ID: {email_id}\"\n",
    "\n",
    "def send_email_tool(recipient:str,subject:str,body:str) -> str:\n",
    "    \"\"\"Sample function to send an email.\"\"\"\n",
    "    return f\"Email send to {recipient} with subject '{subject}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f13534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[read_email_tool,send_email_tool], \n",
    "    checkpointer=InMemorySaver(),\n",
    "    middleware=[\n",
    "        HumanInTheLoopMiddleware(\n",
    "            interrupt_on={\n",
    "                \"send_email_tool\":{\n",
    "                    \"allowed_decisions\":[\"approved\",\"edit\",\"reject\"]\n",
    "                },\n",
    "                \"read_email_tool\":False,\n",
    "            }      \n",
    "            \n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fa1fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"thread_id\":\"test-approve\"}}\n",
    "\n",
    "#Step1: Request \n",
    "result = agent.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"Send email to abc@example.com with subject 'Hello' and body 'How are you?'\")]},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "204ed6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"Send email to abc@example.com with subject 'Hello' and body 'How are you?'\", additional_kwargs={}, response_metadata={}, id='dc08edcb-93c6-4ed2-be72-2eabebf1f153'),\n",
       "  AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 97, 'total_tokens': 125, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bbc38b4db', 'id': 'chatcmpl-Crbagat8JYbgaEf7Clr67QcX0jRDO', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b62f7-13bd-7093-86c6-2ffcd400bfd3-0', tool_calls=[{'name': 'send_email_tool', 'args': {'recipient': 'abc@example.com', 'subject': 'Hello', 'body': 'How are you?'}, 'id': 'call_eUXyzCeaKkTdJMp7r41eEa6h', 'type': 'tool_call'}], usage_metadata={'input_tokens': 97, 'output_tokens': 28, 'total_tokens': 125, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})],\n",
       " '__interrupt__': [Interrupt(value={'action_requests': [{'name': 'send_email_tool', 'args': {'recipient': 'abc@example.com', 'subject': 'Hello', 'body': 'How are you?'}, 'description': \"Tool execution requires approval\\n\\nTool: send_email_tool\\nArgs: {'recipient': 'abc@example.com', 'subject': 'Hello', 'body': 'How are you?'}\"}], 'review_configs': [{'action_name': 'send_email_tool', 'allowed_decisions': ['approved', 'edit', 'reject']}]}, id='3e0f39d79447cafd2a4ed7b353733f10')]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3368b98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paused ! Approving...\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Invalid 'messages': empty array. Expected an array with minimum length 1, but got an empty array instead.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'empty_array'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__interrupt__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Paused ! Approving...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m result \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m      8\u001b[0m     Command(\n\u001b[0;32m      9\u001b[0m         resume\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     10\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecisions\u001b[39m\u001b[38;5;124m\"\u001b[39m:[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapprove\u001b[39m\u001b[38;5;124m\"\u001b[39m}],\n\u001b[0;32m     11\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     12\u001b[0m         }\n\u001b[0;32m     13\u001b[0m     ),\n\u001b[0;32m     14\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\venka\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\main.py:3050\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[0;32m   3047\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m Any] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   3048\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 3050\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m   3051\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3052\u001b[0m     config,\n\u001b[0;32m   3053\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[0;32m   3054\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   3055\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3056\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[0;32m   3057\u001b[0m     print_mode\u001b[38;5;241m=\u001b[39mprint_mode,\n\u001b[0;32m   3058\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[0;32m   3059\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[0;32m   3060\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[0;32m   3061\u001b[0m     durability\u001b[38;5;241m=\u001b[39mdurability,\n\u001b[0;32m   3062\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3063\u001b[0m ):\n\u001b[0;32m   3064\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   3065\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\venka\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\main.py:2633\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[0;32m   2631\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[0;32m   2632\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 2633\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[0;32m   2634\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[0;32m   2635\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m   2636\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[0;32m   2637\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[0;32m   2638\u001b[0m ):\n\u001b[0;32m   2639\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[0;32m   2640\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[0;32m   2641\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[0;32m   2642\u001b[0m     )\n\u001b[0;32m   2643\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[1;32mc:\\Users\\venka\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:167\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[0;32m    165\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 167\u001b[0m     run_with_retry(\n\u001b[0;32m    168\u001b[0m         t,\n\u001b[0;32m    169\u001b[0m         retry_policy,\n\u001b[0;32m    170\u001b[0m         configurable\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    171\u001b[0m             CONFIG_KEY_CALL: partial(\n\u001b[0;32m    172\u001b[0m                 _call,\n\u001b[0;32m    173\u001b[0m                 weakref\u001b[38;5;241m.\u001b[39mref(t),\n\u001b[0;32m    174\u001b[0m                 retry_policy\u001b[38;5;241m=\u001b[39mretry_policy,\n\u001b[0;32m    175\u001b[0m                 futures\u001b[38;5;241m=\u001b[39mweakref\u001b[38;5;241m.\u001b[39mref(futures),\n\u001b[0;32m    176\u001b[0m                 schedule_task\u001b[38;5;241m=\u001b[39mschedule_task,\n\u001b[0;32m    177\u001b[0m                 submit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit,\n\u001b[0;32m    178\u001b[0m             ),\n\u001b[0;32m    179\u001b[0m         },\n\u001b[0;32m    180\u001b[0m     )\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\venka\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39minvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32mc:\\Users\\venka\\anaconda3\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:656\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    654\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m--> 656\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\venka\\anaconda3\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:400\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    398\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 400\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\venka\\anaconda3\\Lib\\site-packages\\langchain\\agents\\factory.py:1131\u001b[0m, in \u001b[0;36mcreate_agent.<locals>.model_node\u001b[1;34m(state, runtime)\u001b[0m\n\u001b[0;32m   1118\u001b[0m request \u001b[38;5;241m=\u001b[39m ModelRequest(\n\u001b[0;32m   1119\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   1120\u001b[0m     tools\u001b[38;5;241m=\u001b[39mdefault_tools,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     runtime\u001b[38;5;241m=\u001b[39mruntime,\n\u001b[0;32m   1127\u001b[0m )\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap_model_call_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;66;03m# No handlers - execute directly\u001b[39;00m\n\u001b[1;32m-> 1131\u001b[0m     response \u001b[38;5;241m=\u001b[39m _execute_model_sync(request)\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;66;03m# Call composed handler with base handler\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m     response \u001b[38;5;241m=\u001b[39m wrap_model_call_handler(request, _execute_model_sync)\n",
      "File \u001b[1;32mc:\\Users\\venka\\anaconda3\\Lib\\site-packages\\langchain\\agents\\factory.py:1102\u001b[0m, in \u001b[0;36mcreate_agent.<locals>._execute_model_sync\u001b[1;34m(request)\u001b[0m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request\u001b[38;5;241m.\u001b[39msystem_message:\n\u001b[0;32m   1100\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [request\u001b[38;5;241m.\u001b[39msystem_message, \u001b[38;5;241m*\u001b[39mmessages]\n\u001b[1;32m-> 1102\u001b[0m output \u001b[38;5;241m=\u001b[39m model_\u001b[38;5;241m.\u001b[39minvoke(messages)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name:\n\u001b[0;32m   1104\u001b[0m     output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m name\n",
      "File \u001b[1;32mc:\\Users\\venka\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5548\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5541\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   5542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   5543\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5546\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5547\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m   5549\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5550\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5551\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5552\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\venka\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    392\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AIMessage:\n\u001b[0;32m    393\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAIMessage\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    396\u001b[0m         cast(\n\u001b[0;32m    397\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 398\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    399\u001b[0m                 [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    400\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    401\u001b[0m                 callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    402\u001b[0m                 tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    403\u001b[0m                 metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    404\u001b[0m                 run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    405\u001b[0m                 run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    406\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    407\u001b[0m             )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    408\u001b[0m         )\u001b[38;5;241m.\u001b[39mmessage,\n\u001b[0;32m    409\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\venka\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1117\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m   1110\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1114\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1115\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m   1116\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m-> 1117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\venka\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:927\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    926\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 927\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    928\u001b[0m                 m,\n\u001b[0;32m    929\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    930\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    931\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    932\u001b[0m             )\n\u001b[0;32m    933\u001b[0m         )\n\u001b[0;32m    934\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    935\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\venka\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1221\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1219\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1221\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m   1222\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1223\u001b[0m     )\n\u001b[0;32m   1224\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1225\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\venka\\anaconda3\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1380\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1378\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_response\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1379\u001b[0m         e\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mhttp_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m-> 1380\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_response_headers\n\u001b[0;32m   1383\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1384\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1385\u001b[0m ):\n\u001b[0;32m   1386\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n",
      "File \u001b[1;32mc:\\Users\\venka\\anaconda3\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1375\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1368\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[0;32m   1369\u001b[0m             response,\n\u001b[0;32m   1370\u001b[0m             schema\u001b[38;5;241m=\u001b[39moriginal_schema_obj,\n\u001b[0;32m   1371\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[0;32m   1372\u001b[0m             output_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_version,\n\u001b[0;32m   1373\u001b[0m         )\n\u001b[0;32m   1374\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1375\u001b[0m         raw_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mwith_raw_response\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload)\n\u001b[0;32m   1376\u001b[0m         response \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mparse()\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\venka\\anaconda3\\Lib\\site-packages\\openai\\_legacy_response.py:364\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m extra_headers[RAW_RESPONSE_HEADER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    362\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[1;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\venka\\anaconda3\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\venka\\anaconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1189\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m   1144\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1186\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m not_given,\n\u001b[0;32m   1187\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m   1188\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m-> 1189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m   1190\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1191\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m   1192\u001b[0m             {\n\u001b[0;32m   1193\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m   1194\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m   1195\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[0;32m   1196\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m   1197\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m   1198\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m   1199\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m   1200\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m   1201\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m   1202\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m   1203\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m   1204\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[0;32m   1205\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m   1206\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m   1207\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[0;32m   1208\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m   1209\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_cache_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_cache_key,\n\u001b[0;32m   1210\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_cache_retention\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_cache_retention,\n\u001b[0;32m   1211\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m   1212\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m   1213\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msafety_identifier\u001b[39m\u001b[38;5;124m\"\u001b[39m: safety_identifier,\n\u001b[0;32m   1214\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m   1215\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m   1216\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m   1217\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m   1218\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m   1219\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m   1220\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m   1221\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m   1222\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m   1223\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m   1224\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m   1225\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m   1226\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbosity\u001b[39m\u001b[38;5;124m\"\u001b[39m: verbosity,\n\u001b[0;32m   1227\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_search_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: web_search_options,\n\u001b[0;32m   1228\u001b[0m             },\n\u001b[0;32m   1229\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsStreaming\n\u001b[0;32m   1230\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[0;32m   1231\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsNonStreaming,\n\u001b[0;32m   1232\u001b[0m         ),\n\u001b[0;32m   1233\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m   1234\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m   1235\u001b[0m         ),\n\u001b[0;32m   1236\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m   1237\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1238\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m   1239\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\venka\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1258\u001b[0m     )\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mc:\\Users\\venka\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Invalid 'messages': empty array. Expected an array with minimum length 1, but got an empty array instead.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'empty_array'}}",
      "\u001b[0mDuring task with name 'model' and id '7ff46e80-13d1-7295-79dc-f0ce86d7b5e8'"
     ]
    }
   ],
   "source": [
    "# Approve process \n",
    "from langgraph.types import Command \n",
    "\n",
    "if \"__interrupt__\" in result:\n",
    "    print(\" Paused ! Approving...\")\n",
    "\n",
    "result = agent.invoke(\n",
    "    Command(\n",
    "        resume={\n",
    "            \"decisions\":[{\"type\":\"approve\"}],\n",
    "            \"messages\": result[\"messages\"]\n",
    "        }\n",
    "    ),\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(f\"Result: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6d9ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### You can do same for reject and edit as well..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
