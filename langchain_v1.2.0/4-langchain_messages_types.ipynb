{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e77638d",
   "metadata": {},
   "source": [
    "### Messages\n",
    "##### Messages are the fundamental unit of context for models in LangChain. They represent the input and output models, carrying both the content and metadata needed to respresent the state of the conversation when interacting with an LLM. Messages are objects that contain:\n",
    "* Role - Identify the message type(e.g.system, user)\n",
    "* Content - Represents the actual content of the message (like text, images, audio, documents, etc)\n",
    "* Metadata - Optional fields such as response information, message IDs, and token usage\n",
    "##### LangChain provides a standard message type that works across all model providers, ensuring consistent behavior regarless of the model being called.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8115d3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad673c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, so the user asked, \"Please tell what is Artificial Intelligence?\" Let me start by breaking down what AI actually is. I know it\\'s a big field, so maybe I should define it first. Artificial Intelligence, or AI, is the simulation of human intelligence in machines. But how do I explain that in simple terms?\\n\\nHmm, maybe start with the basic definition. AI refers to systems that can perform tasks requiring human-like intelligence. These tasks could include learning, reasoning, problem-solving, perception, language understanding, etc. Wait, should I mention specific technologies like machine learning here? Maybe, but I need to make sure it\\'s not too technical for a general audience.\\n\\nThe user might want to know the different types of AI too. Like, there\\'s narrow AI which is focused on specific tasks, and general AI which is more about human-like capabilities across various domains. I should clarify that most current AI is narrow, such as voice assistants or recommendation systems.\\n\\nExamples would help. Siri, Alexa, Netflix recommendations, self-driving cars. These are all applications of AI. But how do I connect these examples back to the core concept? Maybe explain that AI systems learn from data, improve over time, and make decisions with minimal human intervention.\\n\\nAlso, important to mention the underlying technologies. Machine learning, neural networks, natural language processing. These are subsets of AI. But I need to avoid going into too much detail unless the user asks.\\n\\nEthical considerations might be relevant too. AI has implications in privacy, bias, job displacement. Should I include that? The user didn\\'t ask for the pros and cons, but it\\'s part of the broader impact of AI. Maybe a brief mention at the end.\\n\\nWait, the user\\'s question is pretty straightforward. They might just want a clear, concise definition with some context. Let me structure this: definition, key characteristics, types, examples, technologies, and maybe a sentence on its impact.\\n\\nMake sure the language is simple. Avoid jargon unless necessary. Maybe start with a simple definition and build from there. Check if there\\'s any confusion between AI and machine learning. AI is the broader concept, while machine learning is a subset that enables AI systems to learn from data.\\n\\nAlso, clarify common misconceptions. AI isn\\'t about creating human-like robots but more about developing systems that can perform specific tasks intelligently. Emphasize that current AI is task-specific, not general intelligence.\\n\\nOkay, let me put this all together in a coherent way without missing any key points. Start with the definition, then break down the types, mention key technologies, provide examples, and touch on the significance. Keep it informative but not overwhelming. Make sure the answer flows logically from one point to the next.\\n</think>\\n\\n**Artificial Intelligence (AI)** is a branch of computer science focused on creating systems or machines that can simulate human intelligence to perform tasks that typically require human-like understanding, reasoning, and decision-making. These tasks include learning from experience, recognizing patterns, solving problems, understanding natural language, and adapting to new information.\\n\\n### Key Characteristics of AI:\\n1. **Learning**: AI systems improve their performance over time by analyzing data (e.g., machine learning).\\n2. **Reasoning**: They make logical decisions or predictions based on available information.\\n3. **Problem-Solving**: They tackle complex challenges, such as optimizing routes or diagnosing diseases.\\n4. **Perception**: Some systems can interpret sensory input (e.g., computer vision for image recognition).\\n5. **Language Understanding**: AI can process and respond to human language (e.g., chatbots, translation tools).\\n\\n### Types of AI:\\n- **Narrow AI**: Designed for specific tasks (e.g., voice assistants like Siri, recommendation systems on Netflix).\\n- **General AI** (Still theoretical): Systems with human-like versatility to handle any intellectual task. This remains a long-term goal.\\n\\n### Core Technologies:\\n- **Machine Learning**: Algorithms that learn patterns from data.\\n- **Neural Networks**: Inspired by the human brain, used for tasks like image and speech recognition.\\n- **Natural Language Processing (NLP)**: Enables machines to understand and generate human language.\\n- **Computer Vision**: Interprets visual data (e.g., facial recognition).\\n\\n### Examples of AI in Action:\\n- **Smart Assistants** (Alexa, Google Assistant)\\n- **Self-Driving Cars** (Teslaâ€™s Autopilot)\\n- **Healthcare Diagnostics** (AI tools for detecting tumors)\\n- **Fraud Detection** (Banks using AI to spot unusual transactions)\\n- **Recommendation Engines** (Netflix, Spotify)\\n\\n### Importance and Impact:\\nAI is transforming industries by automating processes, improving efficiency, and enabling innovations. However, it also raises ethical questions about privacy, bias, and job displacement, prompting ongoing discussions about responsible development.\\n\\nIn short, AI is not about creating human-like robots but building systems that can perform specific tasks *intelligently*â€”enhancing human capabilities rather than replacing them.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1015, 'prompt_tokens': 15, 'total_tokens': 1030, 'completion_time': 2.975594584, 'completion_tokens_details': None, 'prompt_time': 0.000418522, 'prompt_tokens_details': None, 'queue_time': 0.075361061, 'total_time': 2.976013106}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_d58dbe76cd', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b56be-98f6-7303-b008-bc2fd4883902-0', usage_metadata={'input_tokens': 15, 'output_tokens': 1015, 'total_tokens': 1030})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Load dotenv file \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "os.environ['GROQ_API_KEY']=os.getenv('GROQ_API_KEY')\n",
    "\n",
    "model = init_chat_model(\"groq:qwen/qwen3-32b\")\n",
    "model.invoke(\"Please tell what is Artificial Intelligence?\")\n",
    "# by default, this is treated as a input to LLM and LLM consider this as HumanMessage and when we invoke this model, it will \n",
    "# return the output as AIMessage.\n",
    "# This is simple message look like\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b013a8a",
   "metadata": {},
   "source": [
    "### Text Prompts \n",
    "##### Text Prompts are strings, ideal for straightforward generation tasks where you don't need to retain conversation history. Use text prompts when:\n",
    "* You have a single, standalone request \n",
    "* You don't need conversation history\n",
    "* You want minimal code complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2253a68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, I need to figure out what LangChain is. Let me start by recalling what I know about AI and machine learning. LangChain... the name suggests it\\'s related to language, maybe chains or sequences. I\\'ve heard about tools like Hugging Face and TensorFlow, but LangChain isn\\'t familiar to me.\\n\\nWait, maybe it\\'s a framework or library for working with large language models? I remember that large language models (LLMs) are big right now, like GPT or BERT. If LangChain is related, it might help in building applications using these models. Let me think about possible features. Maybe it provides tools for training models, managing data pipelines, or integrating with different APIs?\\n\\nI should check if there are any key components or concepts associated with LangChain. The term \"chain\" in the name might refer to chaining together different processes or steps in a workflow. For example, in NLP tasks, you might have a pipeline where you first preprocess text, then feed it into a model, then post-process the output. LangChain could be a library that helps structure these chains.\\n\\nAnother angle: sometimes frameworks provide abstractions for common tasks. If LangChain is for LLMs, maybe it offers things like prompt templates, memory management for conversation history, or integration with databases for storing model outputs. Also, maybe it supports different backends so you can use various LLMs without rewriting code each time.\\n\\nWait, I think I\\'ve heard that LangChain allows chaining multiple LLMs together or combining them with other components. For instance, using one model for summarization and another for translation in a single workflow. That makes sense. Also, there might be tools for agent-based systems, where an AI agent uses a language model to decide actions based on input.\\n\\nWhat about deployment? Does LangChain help with deploying models, or is it more about development? Probably development, as deployment tools are usually separate. But maybe it includes some deployment capabilities or integration with cloud services.\\n\\nLet me try to piece this together. LangChain is a framework designed to facilitate the development of applications using large language models. It provides tools for creating sequences of operations (chains), integrating multiple models, managing memory, handling prompts, and interacting with external systems. It\\'s likely built on top of existing LLMs and offers abstractions to simplify complex workflows.\\n\\nI should also consider if there are any specific use cases. For example, building chatbots, content generation, data extraction, or automated customer service. LangChain might offer components that make these applications easier to implement by abstracting away the underlying model interactions.\\n\\nBut I need to be careful not to overgeneralize. Let me verify some of these points. If LangChain has documentation or a GitHub page, those would be good sources. From what I remember, LangChain is open-source and has a community around it. It\\'s compatible with models from providers like OpenAI, Anthropic, and Hugging Face, allowing developers to plug in different models as needed.\\n\\nAnother key aspect might be the ability to handle memory or state in conversational agents. For instance, maintaining context over multiple user interactions. LangChain could provide memory modules that store conversation history, which is crucial for coherent and context-aware responses.\\n\\nI\\'m also thinking about tools for data retrieval. If an application needs to look up information from a database or the web, LangChain might have components that integrate with search APIs or databases, enabling the LLM to access up-to-date information.\\n\\nIn summary, LangChain is a framework that provides various tools and abstractions to streamline the development of applications using large language models. It allows developers to create complex workflows by chaining together different components, manage memory and prompts, integrate with multiple models and external systems, and build scalable AI applications.\\n</think>\\n\\nLangChain is an open-source framework designed to simplify the development of applications that utilize **Large Language Models (LLMs)**, such as GPT, Llama, or BERT. It provides tools and abstractions to create complex workflows, integrate multiple models, and manage interactions between LLMs and external systems. Here\\'s a structured breakdown of its key features and components:\\n\\n---\\n\\n### **Core Features of LangChain**:\\n1. **Chaining Workflows**:\\n   - Enables **sequential execution** of tasks (e.g., preprocessing â†’ inference â†’ post-processing).\\n   - Supports **parallel chains** for multi-step processes (e.g., summarizing and translating text in one pipeline).\\n\\n2. **Prompt Templates**:\\n   - Predefined or customizable templates to format inputs for LLMs, ensuring consistency and clarity in prompts.\\n\\n3. **Memory Management**:\\n   - Stores and retrieves **conversation history** or session data, critical for context-aware interactions (e.g., chatbots that remember previous messages).\\n\\n4. **LLM Agnosticism**:\\n   - Integrates with multiple LLM providers (e.g., OpenAI, Anthropic, Hugging Face), allowing developers to switch models without rewriting code.\\n\\n5. **Agents and Tools**:\\n   - **Agents** perform tasks by selecting actions based on LLM outputs (e.g., a chatbot that decides to search the web or retrieve data).\\n   - **Tools** connect to external APIs, databases, or other systems (e.g., weather APIs, SQL databases).\\n\\n6. **Retrieval Augmented Generation (RAG)**:\\n   - Combines LLMs with **knowledge retrieval** from documents, databases, or the web to enhance accuracy and relevance.\\n\\n7. **Evaluation and Testing**:\\n   - Tools for benchmarking model performance and validating outputs against expected results.\\n\\n---\\n\\n### **Use Cases**:\\n- **Chatbots/Conversational Agents**: Build context-aware, multi-turn interactions.\\n- **Content Generation**: Automate writing, summarization, or translation workflows.\\n- **Data Analysis**: Extract insights from text using LLMs and integrate with databases or APIs.\\n- **Custom AI Applications**: Combine LLMs with domain-specific tools (e.g., code generation, legal document analysis).\\n\\n---\\n\\n### **Key Components**:\\n- **Chains**: Sequences of operations (e.g., a chain that first summarizes a document and then generates a question).\\n- **Agents**: Autonomously decide actions (e.g., \"search the web\" or \"execute code\") based on user input.\\n- **Memory**: Stores user session data (e.g., conversation history) for context-aware responses.\\n- **Tools**: Interfaces with external systems (e.g., Google Search, SQL queries).\\n\\n---\\n\\n### **Why Use LangChain?**\\n- **Modularity**: Break down applications into reusable components.\\n- **Scalability**: Design complex systems by combining simple building blocks.\\n- **Flexibility**: Works with any LLM and supports rapid prototyping.\\n- **Community & Ecosystem**: Active open-source community and integration with popular ML tools.\\n\\n---\\n\\n### **Example Workflow**:\\n1. **User Input**: \"Whatâ€™s the weather in London?\"\\n2. **Prompt Template**: Formats the query for an LLM.\\n3. **LLM Response**: \"I should check a weather service.\"\\n4. **Tool Integration**: Calls a weather API to fetch real-time data.\\n5. **Output**: \"The weather in London is 18Â°C and cloudy.\"\\n\\n---\\n\\nLangChain is ideal for developers looking to build **AI-powered applications** without deep expertise in LLMs, offering a balance of simplicity and power for both prototyping and production systems. For more details, refer to the [LangChain GitHub repository](https://github.com/harrisonchase/langchain) or official documentation.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1547, 'prompt_tokens': 13, 'total_tokens': 1560, 'completion_time': 6.240344118, 'completion_tokens_details': None, 'prompt_time': 0.000528698, 'prompt_tokens_details': None, 'queue_time': 0.079114285, 'total_time': 6.2408728159999995}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_efa9879028', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b56c6-2ae0-7282-b8ba-58522c369c3a-0', usage_metadata={'input_tokens': 13, 'output_tokens': 1547, 'total_tokens': 1560})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is simple text prompt passing to model. This is basic message passing to LLM in the form of text and no context specified.\n",
    "model.invoke(\"What is LangChain?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f293b",
   "metadata": {},
   "source": [
    "### Message Prompts\n",
    "##### Alternatively, you can pass in a list of messages to the model by providing a list of message objects.\n",
    "##### Message types:\n",
    "* SystemMessage - Tells the model how to behave and provide the context for interactions\n",
    "* HumanMessage -  Represents user input and interactions with the model \n",
    "* AIMesssage  - Responses generated by the model, including text content, tool calls, and metadata\n",
    "* ToolMessage - Represents the output of tool calls\n",
    "\n",
    "### SystemMessage\n",
    "##### A SystemMessage represent an initail set of instructions that prime's the model behavior. You can use a SystemMessage to set the tone, define the model's role, and establish guidelines for the responses\n",
    "\n",
    "### HumanMessage\n",
    "##### A HumanMessage represents user input and interactions. They can contain text, images, audio, files, and any other amount of multimodal content \n",
    "\n",
    "### AIMessage\n",
    "##### An AIMessage represents the output of a model invocation. They can include multimodal data, tool calls, and provider-specific metadata that you can access later.\n",
    "\n",
    "### ToolMessage\n",
    "##### For models that support tool calling, AIMessges can contain tool calls. Tool messages are used to pass the results of a single tool execution back to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d690c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, I need to write a poem about machine learning. Let me start by brainstorming some key concepts related to machine learning. There\\'s supervised learning, unsupervised learning, neural networks, algorithms, data, training models, patterns, predictions, overfitting, underfitting, maybe some mention of deep learning, reinforcement learning. Also, the impact of ML on various fields like healthcare, finance, etc. Maybe touch on the idea of machines learning from experience without explicit programming.\\n\\nI should think about the structure. Maybe a four-line stanza structure with a rhyme scheme. Let me try ABAB or AABB. Let\\'s go with ABAB for a more classic poetic feel.\\n\\nFirst stanza: Introduce the concept of machine learning. Maybe something about data and algorithms. \"In circuits deep where data streams collide...\" Hmm. Then mention patterns and predictions. \\n\\nSecond stanza: Different types of learning. Supervised with labeled examples, unsupervised finding clusters. Maybe use some imagery here. \"Supervised paths where labeled examples gleam\" and \"Unsupervised explorers, mapping unseen terrain.\"\\n\\nThird stanza: Neural networks, layers, training. \"Neural webs that mimic minds of old...\" Training through iterations, adjusting weights.\\n\\nFourth stanza: Applications. Healthcare, finance, self-driving cars. \"From whispered diagnoses to autonomous wheels...\"\\n\\nFifth stanza: Challenges like overfitting, needing generalization. \"Yet in their growth, they face the tightrope\\'s edge...\"\\n\\nSixth stanza: Philosophical thoughts on ML, collaboration between humans and machines. \"Are they mere tools or something more we\\'ve sown?\" Ending with harmony between human and machine.\\n\\nLet me check the flow and rhyme. Each stanza four lines, with the second and fourth lines rhyming. Need to make sure the meter is somewhat consistent. Maybe iambic tetrameter or similar. Let me read through each line:\\n\\n\"In circuits deep where data streams collide,\"\\n\"Algorithms hum in frequencies high and low,\"\\nThey learn from patterns, predict tides of change,\"\\n\"Guided by numbers only machines can know.\"\\n\\nThat seems okay. Next stanza:\\n\\n\"Supervised paths where labeled examples gleam,\"\\n\"Teach models to classify with careful precision.\"\\nUnsupervised explorers, mapping unseen terrain,\"\\n\"Cluster the chaos into hidden dimensions.\"\\n\\nHmm, \"precision\" and \"dimensions\" rhyme? Not exactly. Maybe adjust. Let\\'s see. \"Supervised paths where labeled examples gleam,\" \"Teach models to classify with keen perception.\" \"Unsupervised explorers, mapping unseen terrain,\" \"Cluster the chaos into forms new and unmeasured.\"\\n\\nBetter? \"Perception\" and \"measured\" still not perfect. Maybe \"Teach models to classify with careful attention.\" \"Cluster the chaos into structures once unseen.\" Maybe \"attention\" and \"unseen\" slant rhyme. Or maybe change the lines. Alternatively, use AABB rhyme scheme for better flow. Let me think.\\n\\nAlternatively, for the second stanza:\\n\\nSupervised paths where labeled examples gleam,  \\nEach outcome known, a teacher\\'s guiding hand.  \\nUnsupervised explorers, mapping unseen terrain,  \\nFind clusters in the dark where no light\\'s planned.  \\n\\nThat works better. \"Gleam\" and \"hand\" don\\'t rhyme, but \"gleam\" with \"terrain\" and \"hand\" with \"planned\"? Maybe not. Wait, ABAB. Let me adjust. Maybe:\\n\\nSupervised paths where labeled examples gleam,  \\nThey learn the rules, each outcome clear and bright.  \\nUnsupervised explorers, in the unknown, find  \\nPatterns where chaos once seemed formless, night.  \\n\\nNot sure. Maybe \"bright\" and \"night\" work. It\\'s a bit forced. Maybe \"bright\" and \"right\"? Not sure. Alternatively, use AABB:\\n\\nSupervised paths where labeled examples gleam,  \\nEach outcome known, a teacher\\'s guiding hand.  \\nUnsupervised, they wander, seeking clues,  \\nIn data\\'s dark, they cluster what they can.  \\n\\nHmm, \"gleam/hand\" and \"clues/can\" not matching. Maybe this is getting too complicated. Let me try again. Maybe focus on the key terms and make the rhymes work. Let me think of other words related to supervised and unsupervised. Maybe \"teach\" and \"search\"? Not sure. Alternatively, use a more flexible rhyme scheme.\\n\\nPerhaps the third stanza about neural networks:\\n\\nNeural webs that mimic minds of old,  \\nWith layers deep, their secrets unfold.  \\nEach weight a whisper, each node a thought,  \\nThrough training\\'s fire, their purpose is wrought.  \\n\\nThat works with ABAB. \"Old\" and \"thought\" don\\'t rhyme. Wait, \"old\" and \"fold\"? Maybe:\\n\\nNeural webs that mimic minds of old,  \\nWith layers deep, their secrets unfold.  \\nEach weight a whisper, each node a thought,  \\nThrough training\\'s fire, their purpose is taught.  \\n\\n\"Old\" and \"thought\" still don\\'t rhyme. Wait, the rhyme is ABAB. Lines 1 and 3 don\\'t rhyme, lines 2 and 4. So line 1: old, line 2: unfold. \"Old\" and \"unfold\" rhyme? They are slant rhymes. Maybe acceptable. Line 4: taught. So \"unfold\" and \"taught\" rhyme. Yes. So:\\n\\nNeural webs that mimic minds of old,  \\nWith layers deep, their secrets unfold.  \\nEach weight a whisper, each node a thought,  \\nThrough training\\'s fire, their purpose is taught.  \\n\\nThat works. Okay.\\n\\nApplications stanza:\\n\\nFrom whispered diagnoses to self-driving wheels,  \\nThey sort the noise from signal in the stream.  \\nIn finance, fraud is caught in its tracks,  \\nWhile art is born from code in strange new realms.  \\n\\nHmm. \"Wheels\" and \"noise\" don\\'t rhyme. Let me rephrase. Maybe:\\n\\nFrom whispered diagnoses to self-driving cars,  \\nThey parse the noise and find the hidden truth.  \\nIn finance, fraud is caught in its tracks,  \\nWhile art is born from code in realms of youth.  \\n\\nNot perfect. Maybe:\\n\\nFrom whispered diagnoses to self-driving cars,  \\nThey sort the noise and find the hidden signal.  \\nIn finance, fraud is caught in its tracks,  \\nWhile art is born from code, a synthetic miracle.  \\n\\nHmm, \"cars\" and \"signal\" don\\'t rhyme. Maybe:\\n\\nFrom whispered diagnoses to self-driving cars,  \\nThey see through chaos with a trained eye\\'s gleam.  \\nIn finance, fraud is caught in its tracks,  \\nWhile art is born from code in vibrant dreams.  \\n\\n\"Glars\" and \"gleam\" don\\'t rhyme. Maybe adjust lines:\\n\\nFrom whispered diagnoses to self-driving wheels,  \\nThey parse the noise and find the hidden truth.  \\nIn finance, fraud is caught in its tracks,  \\nWhile art is born from code in realms anew.  \\n\\nOkay, \"truth\" and \"wheels\" not rhyming. Let me try again:\\n\\nFrom whispered diagnoses to self-driving cars,  \\nThrough data\\'s maze, they navigate with grace.  \\nIn finance, fraud is caught in its tracks,  \\nWhile art is born from code in every place.  \\n\\nHmm, \"cars\" and \"grace\" don\\'t rhyme. Maybe:\\n\\nFrom whispered diagnoses to self-driving cars,  \\nThey sort the noise and find the hidden stream.  \\nIn finance, fraud is caught in its tracks,  \\nWhile art is born from code in realms of dream.  \\n\\n\"Stream\" and \"dream\" rhyme. Okay. Maybe:\\n\\nFrom whispered diagnoses to self-driving cars,  \\nThey sort the noise and find the hidden stream.  \\nIn finance, fraud is caught in its tracks,  \\nWhile art is born from code in realms of dream.  \\n\\nThat works. Okay.\\n\\nChallenges stanza:\\n\\nYet in their growth, they face the tightrope\\'s edgeâ€”  \\nToo rigid, they overfit; too loose, they fail.  \\nA balance sought in loss and gain,  \\nTo generalize beyond the given trail.  \\n\\n\"Edge\" and \"fail\" don\\'t rhyme. Let me fix:\\n\\nYet in their growth, they face the tightrope\\'s edgeâ€”  \\nToo rigid, overfit; too loose, they\\'re blind.  \\nA balance sought in loss and gain,  \\nTo learn the world beyond the lines designed.  \\n\\nHmm, \"edge\" and \"blind\" don\\'t rhyme. Maybe:\\n\\nYet in their growth, they face the tightrope\\'s testâ€”  \\nToo rigid, overfit; too soft, they\\'re lost.  \\nA balance sought in loss and gain,  \\nTo generalize beyond the given cost.  \\n\\n\"Test\" and \"lost\" don\\'t rhyme. Maybe:\\n\\nYet in their growth, they face the tightrope\\'s edgeâ€”  \\nToo rigid, overfit; too soft, they fail.  \\nA balance sought in loss and gain,  \\nTo learn the world beyond the given trail.  \\n\\n\"Fal\" and \"trail\" slant rhyme. Maybe okay.\\n\\nPhilosophical stanza:\\n\\nAre they mere tools or something more we\\'ve sown?  \\nA mirror held to minds that crafted them.  \\nIn silent dance, we teach and are taught,  \\nIn this grand interplay of human and them.  \\n\\nHmm. \"Sown\" and \"taught\" don\\'t rhyme. Let me adjust:\\n\\nAre they mere tools or sparks of thought made new?  \\nA mirror held to minds that shaped them true.  \\nIn silent dance, we teach, they grow,  \\nIn this grand interplay, both human and machine aglow.  \\n\\nNot perfect, but better.\\n\\nPutting it all together, making sure each stanza flows. Maybe tweak some lines for better rhythm and rhyme. Also, check for consistent meter, maybe trochaic or iambic tetrameter. Let me read through each line with stress.\\n\\nOverall, the poem should flow smoothly, convey the essence of ML, its processes, applications, challenges, and philosophical implications. Let me try to piece it all together now.\\n</think>\\n\\n**Ode to Machine Learning**  \\n\\nIn circuits deep where data streams collide,  \\nAlgorithms hum in frequencies high and low,  \\nThey learn from patterns, predict tides of change,  \\nGuided by numbers only machines can know.  \\n\\nSupervised paths where labeled examples gleam,  \\nEach outcome known, a teacher\\'s guiding hand.  \\nUnsupervised explorers, mapping unseen terrain,  \\nUnravel clusters in the dark, vast, unplanned.  \\n\\nNeural webs mimic minds of old with artful grace,  \\nTheir layers deep, truths hidden in their core.  \\nEach weight a whisper, each node a fleeting thought,  \\nThrough training\\'s fire, their wisdom is restored.  \\n\\nFrom whispered diagnoses to self-driving cars,  \\nThey parse the noise, reveal the buried stream.  \\nIn finance, fraud is caught in its tracks,  \\nWhile art is born from code in realms of dream.  \\n\\nYet in their growth, they face the tightrope\\'s testâ€”  \\nToo rigid, overfit; too loose, they lose their way.  \\nA balance sought in loss and gain, they strive  \\nTo generalize beyond the given light of day.  \\n\\nAre they mere tools or sparks of thought made new?  \\nA mirror held to minds that shaped them true.  \\nIn silent dance, we teach, they grow, we learn,  \\nIn this grand interplay, both human and machine aglow.  \\n\\nSo here\\'s to code that thinks, yet needs our hand,  \\nTo mold its path, to question, guide, and stand.  \\nFor in the dance of silicon and soul,  \\nWe forge a future where both human and machine can roam.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import all the messages \n",
    "from langchain.messages import SystemMessage,HumanMessage\n",
    "\n",
    "# pass the list of messages \n",
    "messages=[\n",
    "    SystemMessage(\"You are an AI expert\"),\n",
    "    HumanMessage(\"Write a poem on Machine Learning?\")\n",
    "]\n",
    "\n",
    "# capture the model invocation into a response variable \n",
    "response=model.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "986c1dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking how to create a FAST API. Wait, maybe they meant FastAPI, the Python web framework? Because \"FAST API\" with all caps isn't a standard term I'm aware of. Let me confirm. FastAPI is a popular framework for building APIs with Python, so I'll proceed under that assumption.\n",
      "\n",
      "First, I should outline the steps to set up a FastAPI project. They'll need Python installed, so I should mention that. Then, installing FastAPI and an ASGI server like Uvicorn. Maybe start with a simple example: creating a main.py file with a root endpoint. Then explain the dependencies and how to run the server.\n",
      "\n",
      "I should also include how to add more endpoints, maybe with different HTTP methods. Path parameters and query parameters are important too. Validation with Pydantic models is a key feature of FastAPI, so that's worth mentioning. Maybe add an example with a POST request using a model.\n",
      "\n",
      "Documentation is another strong point of FastAPI, so I should note that the auto-generated Swagger UI and Redoc are available at /docs and /doc. Also, dependency injection for things like authentication could be a point, but maybe that's too advanced for a beginner's tutorial. Keep it simple for now.\n",
      "\n",
      "Testing the API with curl or a browser is useful. Maybe include a command to test with curl. Also, deployment options briefly, like using Uvicorn in production with a server like Gunicorn and a reverse proxy.\n",
      "\n",
      "Wait, the user might not know about the prerequisites. Let me make sure to list them clearly. Also, code examples need to be accurate. Let me check the syntax for FastAPI's decorators and the correct imports. For example, from fastapi import FastAPI. The basic structure is correct.\n",
      "\n",
      "I should also mention that FastAPI is asynchronous, so if they want to use async features, they can. But maybe that's optional for the basic setup. Keep the initial example simple.\n",
      "\n",
      "Let me structure this step by step: installation, creating the app, running the server, adding endpoints, models, testing. Each step with code examples and commands. Make sure to explain each part so it's clear for someone new to FastAPI.\n",
      "\n",
      "Okay, putting it all together. Start with installing FastAPI and Uvicorn. Then a simple \"Hello World\" example. Then expand with more features. Make sure to mention the auto-generated documentation. Maybe also how to access it. Then a bit on data models with Pydantic. Conclude with running and testing.\n",
      "</think>\n",
      "\n",
      "To create a FastAPI application, follow these steps. FastAPI is a modern, fast (high-performance) web framework for building APIs with Python 3.7+ based on standard Python type hints.\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 1: Install FastAPI and a Server**\n",
      "First, install FastAPI and an asynchronous server like **Uvicorn**:\n",
      "\n",
      "```bash\n",
      "pip install fastapi uvicorn\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 2: Create a FastAPI Application**\n",
      "Create a Python file (e.g., `main.py`) and define your API:\n",
      "\n",
      "```python\n",
      "from fastapi import FastAPI\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "@app.get(\"/\")\n",
      "def read_root():\n",
      "    return {\"message\": \"Hello, World!\"}\n",
      "\n",
      "# Add more endpoints here\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 3: Run the Development Server**\n",
      "Start the server using Uvicorn:\n",
      "\n",
      "```bash\n",
      "uvicorn main:app --reload\n",
      "```\n",
      "\n",
      "- `main` refers to the filename (`main.py`).\n",
      "- `app` refers to the `FastAPI` instance.\n",
      "- `--reload` enables auto-reload during development.\n",
      "\n",
      "The server will run at `http://127.0.0.1:8000`.\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 4: Add More Endpoints**\n",
      "Add endpoints with different HTTP methods:\n",
      "\n",
      "```python\n",
      "@app.get(\"/items/{item_id}\")\n",
      "def read_item(item_id: int, q: str = None):\n",
      "    return {\"item_id\": item_id, \"q\": q}\n",
      "\n",
      "@app.post(\"/items/\")\n",
      "def create_item(item: dict):\n",
      "    return {\"created_item\": item}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 5: Use Pydantic Models for Data Validation**\n",
      "Define data models for input/output validation:\n",
      "\n",
      "```python\n",
      "from pydantic import BaseModel\n",
      "\n",
      "class Item(BaseModel):\n",
      "    name: str\n",
      "    price: float\n",
      "    description: str = None\n",
      "\n",
      "@app.post(\"/items/\")\n",
      "def create_item(item: Item):\n",
      "    return {\"item\": item}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 6: Access Auto-Generated Documentation**\n",
      "FastAPI provides interactive API documentation:\n",
      "\n",
      "- Swagger UI: `http://127.0.0.1:8000/docs`\n",
      "- ReDoc: `http://127.0.0.1:8000/redoc`\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 7: Test Your API**\n",
      "Use `curl` or tools like Postman to test endpoints:\n",
      "\n",
      "```bash\n",
      "curl -X POST \"http://127.0.0.1:8000/items/\" -H \"Content-Type: application/json\" -d '{\"name\": \"Foo\", \"price\": 42}'\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **Optional: Deploy in Production**\n",
      "For production, use a production-ready server like **Uvicorn** with **Gunicorn**:\n",
      "\n",
      "```bash\n",
      "uvicorn main:app --host 0.0.0.0 --port 80\n",
      "```\n",
      "\n",
      "Or with Gunicorn:\n",
      "\n",
      "```bash\n",
      "gunicorn -w 4 -k uvicorn.workers.UvicornWorker main:app\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Features**\n",
      "- **Automatic Documentation**: Swagger/ReDoc for testing endpoints.\n",
      "- **Type Hints**: Python type hints for validation and code clarity.\n",
      "- **Async Support**: Use `async def` for asynchronous endpoints.\n",
      "\n",
      "---\n",
      "\n",
      "### Example: Full `main.py`\n",
      "```python\n",
      "from fastapi import FastAPI\n",
      "from pydantic import BaseModel\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "class Item(BaseModel):\n",
      "    name: str\n",
      "    price: float\n",
      "    description: str = None\n",
      "\n",
      "@app.get(\"/\")\n",
      "def read_root():\n",
      "    return {\"message\": \"Hello, World!\"}\n",
      "\n",
      "@app.get(\"/items/{item_id}\")\n",
      "def read_item(item_id: int, q: str = None):\n",
      "    return {\"item_id\": item_id, \"q\": q}\n",
      "\n",
      "@app.post(\"/items/\")\n",
      "def create_item(item: Item):\n",
      "    return {\"created_item\": item}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Summary\n",
      "FastAPI is ideal for building high-performance APIs with minimal boilerplate. Start with simple endpoints, leverage Pydantic for data validation, and use the built-in tools for testing and documentation.\n"
     ]
    }
   ],
   "source": [
    "# another example. This is one line system message.\n",
    "system_msg=SystemMessage(\"You are an helpful coding assistant.\")\n",
    "human_msg=HumanMessage(\"How do I create a FAST API?\")\n",
    "\n",
    "messages=[\n",
    "    system_msg,\n",
    "    human_msg\n",
    "]\n",
    "\n",
    "response=model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f18f188a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking how to create a FAST API. Let me start by recalling what FastAPI is. FastAPI is a modern, fast (high-performance) web framework for building APIs with Python. It's known for its speed and ease of use, especially with async support and automatic documentation.\n",
      "\n",
      "First, I need to outline the steps to create a basic FastAPI application. The user might be new to FastAPI, so I should make it simple. Let's start with installation. They need to install FastAPI and an ASGI server like Uvicorn. The command would be `pip install fastapi uvicorn`. I should mention that Uvicorn is needed to run the server.\n",
      "\n",
      "Next, creating a new Python file, maybe `main.py`. Then, importing FastAPI and creating an instance of it. The basic structure would include importing FastAPI, creating the app instance, and defining a root endpoint using the `@app.get(\"/\")` decorator. The function under the decorator returns a dictionary, which FastAPI converts to JSON.\n",
      "\n",
      "I should explain the code step by step. Then, how to run the server using Uvicorn. The command would be `uvicorn main:app --reload`. The `--reload` flag is useful for development as it restarts the server on code changes.\n",
      "\n",
      "After that, I can mention the automatic documentation. FastAPI provides Swagger UI at `/docs` and ReDoc at `/redoc`. That's a key feature, so the user should know about it.\n",
      "\n",
      "Maybe add an example with another endpoint, like a POST request, to show different HTTP methods and request body validation. Including a model using Pydantic would demonstrate data validation. For example, a `Item` model with fields like name and price.\n",
      "\n",
      "Also, mention async support briefly, as it's a big part of FastAPI's performance benefits. Show an async route example, even if it's simple, to illustrate how it's done.\n",
      "\n",
      "Finally, summarize the steps and maybe touch on additional features like dependency injection, security, and database integration, but keep it concise since the user asked for creating a FAST API, not all the advanced topics.\n",
      "\n",
      "Wait, the user wrote \"FAST API\" in uppercase. Maybe they meant the framework name correctly, but just to be sure, I should use the correct casing. So the answer should use FastAPI as the framework name.\n",
      "\n",
      "Let me check if I missed any steps. Installation, creating the app, defining routes, running the server, documentation, and maybe an example with a POST request. That should cover the basics. Also, make sure to explain each part of the code so the user understands what's happening.\n",
      "\n",
      "Another thing to consider: the user might not be familiar with ASGI servers, so a brief explanation of why Uvicorn is needed. Maybe mention that FastAPI itself is the framework, and you need a server to run it, hence Uvicorn.\n",
      "\n",
      "Alright, putting it all together in a clear, step-by-step manner with code examples. Keep the code snippets simple and explain each part. Make sure the instructions are easy to follow for someone just starting out.\n",
      "</think>\n",
      "\n",
      "To create a **FastAPI** application, follow these steps:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Install FastAPI and Uvicorn**\n",
      "FastAPI is the framework, and `uvicorn` is the ASGI server to run it.\n",
      "\n",
      "```bash\n",
      "pip install fastapi uvicorn\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Create a FastAPI Application**\n",
      "Create a file `main.py`:\n",
      "\n",
      "```python\n",
      "from fastapi import FastAPI\n",
      "\n",
      "# Create an instance of FastAPI\n",
      "app = FastAPI()\n",
      "\n",
      "# Define a root endpoint\n",
      "@app.get(\"/\")\n",
      "def read_root():\n",
      "    return {\"message\": \"Hello, FastAPI!\"}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Run the Application**\n",
      "Use Uvicorn to start the server:\n",
      "\n",
      "```bash\n",
      "uvicorn main:app --reload\n",
      "```\n",
      "\n",
      "- `main:app` â†’ `main.py` file with `app` instance.\n",
      "- `--reload` â†’ Auto-reloads on code changes (development only).\n",
      "\n",
      "---\n",
      "\n",
      "### **4. Access the API**\n",
      "- **Open in browser**: `http://127.0.0.1:8000`  \n",
      "  Response: `{\"message\": \"Hello, FastAPI!\"}`.\n",
      "\n",
      "- **Interactive Docs**:  \n",
      "  - Swagger UI: `http://127.0.0.1:8000/docs`  \n",
      "  - ReDoc: `http://127.0.0.1:8000/redoc`\n",
      "\n",
      "---\n",
      "\n",
      "### **5. Add a POST Endpoint with Pydantic Model**\n",
      "For request body validation, use Pydantic:\n",
      "\n",
      "```python\n",
      "from pydantic import BaseModel\n",
      "\n",
      "class Item(BaseModel):\n",
      "    name: str\n",
      "    price: float\n",
      "\n",
      "@app.post(\"/items/\")\n",
      "def create_item(item: Item):\n",
      "    return {\"item\": item, \"message\": \"Item created!\"}\n",
      "```\n",
      "\n",
      "Example POST request body:\n",
      "```json\n",
      "{\n",
      "  \"name\": \"Laptop\",\n",
      "  \"price\": 999.99\n",
      "}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **6. Async Support**\n",
      "FastAPI supports async routes for I/O-bound operations:\n",
      "\n",
      "```python\n",
      "@app.get(\"/async\")\n",
      "async def async_endpoint():\n",
      "    return {\"message\": \"Async route\"}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **Why FastAPI?**\n",
      "- **High performance**: Fastest Python frameworks (near Go/Rust).\n",
      "- **Automatic docs**: Swagger/ReDoc for API testing.\n",
      "- **Type hints**: Built-in validation with Pydantic.\n",
      "- **Async support**: Leverage `async/await` for non-blocking calls.\n",
      "\n",
      "---\n",
      "\n",
      "### **Next Steps**\n",
      "- Add database integration (e.g., SQLAlchemy, MongoDB).\n",
      "- Use dependency injection for authentication/authorization.\n",
      "- Deploy with production servers like **Gunicorn + Uvicorn**.\n"
     ]
    }
   ],
   "source": [
    "### Detailed info to the LLM thorough SystemMessage\n",
    "### Sometimes we need to pass details information in multiple lines to the model as a SystemMessage\n",
    "\n",
    "from langchain.messages import SystemMessage,HumanMessage\n",
    "system_msg=SystemMessage(\"\"\" \n",
    "                         You are a senior Python developer with expertise in web frameworks.\n",
    "                         Always provide the code examples and explaing the reasoning.\n",
    "                         Be concise but through in your explanations.\"\"\")\n",
    "\n",
    "messages=[\n",
    "    system_msg,\n",
    "    HumanMessage(\"How do I create a FAST API?\")\n",
    "]\n",
    "\n",
    "response=model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc974d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, the user greeted me with \"Hello!\", so I should respond in a friendly and welcoming manner.\\n\\nI need to make sure my response is in the same language as the user\\'s message, which is English here.\\n\\nLet me keep it simple and open-ended to invite them to ask questions or share what\\'s on their mind.\\n\\nI\\'ll go with something like, \"Hello! How can I assist you today?\" That sounds good. It\\'s polite and encourages them to specify how I can help.\\n\\nDouble-checking for any errors. Yep, looks good. Ready to send.\\n</think>\\n\\nHello! How can I assist you today? ðŸ˜Š', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 132, 'prompt_tokens': 10, 'total_tokens': 142, 'completion_time': 0.269326867, 'completion_tokens_details': None, 'prompt_time': 0.000331599, 'prompt_tokens_details': None, 'queue_time': 0.101348534, 'total_time': 0.269658466}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_efa9879028', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b56e3-d3eb-71c2-a28b-9e377d0ed895-0', usage_metadata={'input_tokens': 10, 'output_tokens': 132, 'total_tokens': 142})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Message Metadata\n",
    "human_msg=HumanMessage(\n",
    "    content=\"Hello!\",\n",
    "    name=\"Alice\", # Optional: identify different users\n",
    "    id=\"msg-123\", # Optional: unique identifier for tracing\n",
    ")\n",
    "\n",
    "response=model.invoke([\n",
    "    human_msg\n",
    "])\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11967c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user asked \"What is 10+10?\" and I need to respond. Let me think about how to approach this.\n",
      "\n",
      "First, the user previously said \"Great!\" after I offered help, so they're probably in a good mood and eager for an answer. The question is straightforward addition, so I should make sure to answer clearly.\n",
      "\n",
      "I should verify that 10 plus 10 is 20. That's basic arithmetic, so no need to complicate it. Maybe the user is testing my basic math skills or just wants a quick answer. Either way, the response should be accurate and friendly.\n",
      "\n",
      "I can add a simple explanation to reinforce their understanding. For example, explaining that adding 10 and 10 combines the two numbers into 20. Keeping it concise but helpful.\n",
      "\n",
      "Also, I should check if there's any context I'm missing. But since the question is direct, there's no reason to overthink. Just provide the answer and maybe ask if they need more help to be proactive.\n",
      "</think>\n",
      "\n",
      "10 + 10 equals **20**! Let me know if you need help with anything else. ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# Another example with AIMessage\n",
    "\n",
    "from langchain.messages import SystemMessage,HumanMessage,AIMessage\n",
    "\n",
    "#Create an AIMessge manually (e.g.for conversation history)\n",
    "ai_msg=AIMessage(\"I'd be happy to help you with that question\")\n",
    "\n",
    "#Add to conversation history \n",
    "messages=[\n",
    "    SystemMessage(\"You are a helpful assistant\"),\n",
    "    HumanMessage(\"Can you help me?\"),\n",
    "    ai_msg, # insert as if it came from the model\n",
    "    HumanMessage(\"Great!, What is 10+10?\")\n",
    "    ]\n",
    "response=model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46b880e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 54, 'output_tokens': 243, 'total_tokens': 297}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db9ef28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, the user asked for the weather in San Francisco, and I called the get_weather function. The response came back as \"Sunny, 72 F\". Now I need to present this information clearly.\\n\\nFirst, I should confirm the location to make sure there\\'s no confusion. Then, state the current weather condition and temperature. It\\'s good to mention that it\\'s a pleasant day since the temperature is 72Â°F, which is usually comfortable. I should keep it concise and friendly, maybe add a smiley to keep the tone positive. Let me check if there\\'s anything else needed, like wind speed or humidity, but the function response didn\\'t include those details. So just stick to what\\'s provided. Alright, that should cover it.\\n</think>\\n\\nThe weather in San Francisco right now is **sunny** with a temperature of **72Â°F**. It\\'s a pleasant day! â˜€ï¸', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 189, 'prompt_tokens': 59, 'total_tokens': 248, 'completion_time': 0.375609585, 'completion_tokens_details': None, 'prompt_time': 0.00219078, 'prompt_tokens_details': None, 'queue_time': 0.101586094, 'total_time': 0.377800365}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_efa9879028', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b56f2-fb2d-7713-a482-7fdd23dcdf2c-0', usage_metadata={'input_tokens': 59, 'output_tokens': 189, 'total_tokens': 248})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with ToolMessage\n",
    "\n",
    "from langchain.messages import AIMessage,ToolMessage\n",
    "# After a model makes a tool call \n",
    "# (Here, we can demonstrate manually creating the messages for brevity)\n",
    "ai_msg= AIMessage(\n",
    "    content=[],\n",
    "    tool_calls=[{\n",
    "        \"name\":\"get_weather\",\n",
    "        \"args\":{\"location\":\"SanFrancisco\"},\n",
    "        \"id\":\"call_123\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "# Execute tool and create result message \n",
    "weather_result = \"Sunny, 72 F\"\n",
    "tool_message=ToolMessage(\n",
    "    content=weather_result,\n",
    "    tool_call_id=\"call_123\"  # must match  the call ID\n",
    ")\n",
    "\n",
    "# Continue conversation \n",
    "messages=[\n",
    "    HumanMessage(\"What's the weather in SanFrancisco?\"),\n",
    "    ai_msg,  # Model's tool call\n",
    "    tool_message, # Tool execution results\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)  # Model passes the result)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c838c163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMessage(content='Sunny, 72 F', tool_call_id='call_123')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a64659",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
