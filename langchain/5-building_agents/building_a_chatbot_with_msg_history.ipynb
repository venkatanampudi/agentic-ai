{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc8e381d",
   "metadata": {},
   "source": [
    "### Building a chatbot with Message History\n",
    "##### Objective:How to design and implement an LLM-powered chatbot. This chatbot will be able to have a conversation and remember the previous interactions.\n",
    "##### This chatbot will only use the language models to have a conversation. There are several other related concepts that you may look for:\n",
    "* Conversational RAG: Enable a chatbot experience over an external source of data\n",
    "* Agents: Build a chatbot that can take actions \n",
    "* install \"pip install langchain_community\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73f2d72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\venka\\python\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={}, client=<groq.resources.chat.completions.Completions object at 0x00000277E422D310>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000277E556E0F0>, model_name='groq/compound', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"groq/compound\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71de90b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Venkat! Nice to meet you. How can I assist you today?', additional_kwargs={'reasoning_content': '<Think>\\n\\n</Think>'}, response_metadata={'token_usage': {'completion_tokens': 73, 'prompt_tokens': 258, 'total_tokens': 331, 'completion_time': 0.160243, 'completion_tokens_details': None, 'prompt_time': 0.01034, 'prompt_tokens_details': None, 'queue_time': 0.164565, 'total_time': 0.170583}, 'model_name': 'groq/compound', 'system_fingerprint': None, 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b46e7-0644-7301-b210-1f32dd864dac-0', usage_metadata={'input_tokens': 258, 'output_tokens': 73, 'total_tokens': 331})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi,My name is Venkat and I am a Chief AI Engineer\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf6bbce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='**Answer to your original question – “Hey, what’s my name and what do I do?”**\\n\\n---\\n\\n### Information from the conversation\\n1. **User’s statement:**  \\n   *“Hi, My name is Venkat and I am a Chief AI Engineer.”*  \\n   This was provided in the very first message you sent.\\n\\n2. **Interpretation:**  \\n   - **Name:** Venkat  \\n   - **Profession / Role:** Chief AI Engineer  \\n\\n3. **Verification:**  \\n   There have been no subsequent messages that contradict or modify this information. Therefore, the most reliable data we have about you remains the initial self‑identification.\\n\\n---\\n\\n### Final response\\n- **Your name:** **Venkat**  \\n- **What you do:** **You are a Chief AI Engineer** (i.e., you lead AI engineering efforts, oversee model development, architecture design, and the deployment of AI systems within your organization).\\n\\n---\\n\\nIf you need anything else—whether it’s technical advice, brainstorming AI project ideas, or anything related to your role—just let me know!', additional_kwargs={'reasoning_content': \"\\nI will retrieve the information about the user's name and profession from the previous conversation turns.\\n\\nThe user mentioned earlier that their name is Venkat and they are a Chief AI Engineer.\\n\\n\\nYour name is Venkat and you are a Chief AI Engineer.\"}, response_metadata={'token_usage': {'completion_tokens': 362, 'prompt_tokens': 1062, 'total_tokens': 1424, 'completion_time': 0.788942, 'completion_tokens_details': None, 'prompt_time': 0.036477, 'prompt_tokens_details': None, 'queue_time': 0.150396, 'total_time': 0.825418}, 'model_name': 'groq/compound', 'system_fingerprint': None, 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b46e7-1624-7571-989d-ed8cd9919144-0', usage_metadata={'input_tokens': 1062, 'output_tokens': 362, 'total_tokens': 1424})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi,My name is Venkat and I am a Chief AI Engineer\"),\n",
    "        AIMessage(content=\"Hello Venkat! Great to meet you. How can I assist you today?\"),\n",
    "        HumanMessage(content=\"Hey What's my name and what do I do?\") \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a37fb21",
   "metadata": {},
   "source": [
    "### Message History \n",
    "##### We can use a Message History class to wrap our model and make it \"stateful\". This will keep track of inputs and outputs of the model,and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aec00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory  # This is for chain\n",
    "\n",
    "# to store chat message history with session_id. To distinguish between chats, session_id is the primary key\n",
    "store={} \n",
    "\n",
    "# Create a function for session history\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# use this variable,we can interact with LLM based on chat history using RunnableWithMessageHistory\n",
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05221142",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7c8b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create configuration. Here session_id is \"chat1\"\n",
    "\n",
    "config={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4bc546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this session id to chat with LLM model \n",
    "response=with_message_history.invoke(\n",
    "\n",
    "    [HumanMessage(content=\"Hi,My name is Venkat and I'm a AI Engineer.I live in New Jersey\")],\n",
    "     config=config\n",
    "     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "827127a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Answer –\\u202fYour name is\\u202fVenkat.**\\n\\n---\\n\\n## How I arrived at that answer (full reasoning)\\n\\n1. **Your original self‑identification**  \\n   - In the very first message you wrote:  \\n     > “Hi, My name is **Venkat** and I am a Chief AI Engineer.”  \\n   - This gave an explicit, unambiguous declaration of your name.\\n\\n2. **Subsequent clarification (still the same name)**  \\n   - Later you said:  \\n     > “Hi, My name is **Venkat**, I am a AI Engineer and I live in New Jersey.”  \\n   - The name **Venkat** appears again, confirming it has not changed.\\n\\n3. **Instruction to remember**  \\n   - You asked me to “remember” that exact phrasing as the definitive answer to the *initial question* “What’s my name?”.  \\n   - I stored the three pieces of information you gave me (name\\u202f=\\u202fVenkat, role\\u202f=\\u202fAI Engineer/Chief AI Engineer, location\\u202f=\\u202fNew\\u202fJersey).\\n\\n4. **Consistent retrieval in later interactions**  \\n   - Whenever you later asked “What’s my name?” I retrieved the stored value and responded **Venkat**.  \\n   - Each of those responses was reinforced by you repeating the same details, confirming that no contradictory information was introduced.\\n\\n5. **No contradictory updates**  \\n   - Throughout the conversation you never supplied a different name or indicated a change, so the only valid answer remains **Venkat**.\\n\\n---\\n\\n### Summary\\n\\n- **Name:**\\u202fVenkat  \\n- **Profession:**\\u202fAI Engineer (also referred to as Chief AI Engineer)  \\n- **Location:**\\u202fNew\\u202fJersey  \\n\\nBecause the name was directly supplied by you, and you explicitly asked me to remember it as the answer to the initial question, the correct answer to **“What’s my name?”** is **Venkat**.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d1ff248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Answer –\\u202fYour name is\\u202fVenkat.**\\n\\n---\\n\\n## How I arrived at that answer (full reasoning)\\n\\n1. **Your original self‑identification**  \\n   - In the very first message you wrote:  \\n     > “Hi, My name is **Venkat** and I am a Chief AI Engineer.”  \\n   - This gave an explicit, unambiguous declaration of your name.\\n\\n2. **Subsequent clarification (still the same name)**  \\n   - Later you said:  \\n     > “Hi, My name is **Venkat**, I am a AI Engineer and I live in New Jersey.”  \\n   - The name **Venkat** appears again, confirming it has not changed.\\n\\n3. **Instruction to remember**  \\n   - You asked me to “remember” that exact phrasing as the definitive answer to the *initial question* “What’s my name?”.  \\n   - I stored the three pieces of information you gave me (name\\u202f=\\u202fVenkat, role\\u202f=\\u202fAI Engineer/Chief AI Engineer, location\\u202f=\\u202fNew\\u202fJersey).\\n\\n4. **Consistent retrieval in later interactions**  \\n   - Whenever you later asked “What’s my name?” I retrieved the stored value and responded **Venkat**.  \\n   - Each of those responses was reinforced by you repeating the same details, confirming that no contradictory information was introduced.\\n\\n5. **No contradictory updates**  \\n   - Throughout the conversation you never supplied a different name or indicated a change, so the only valid answer remains **Venkat**.\\n\\n---\\n\\n### Summary\\n\\n- **Name:**\\u202fVenkat  \\n- **Profession:**\\u202fAI Engineer (also referred to as Chief AI Engineer)  \\n- **Location:**\\u202fNew\\u202fJersey  \\n\\nBecause the name was directly supplied by you, and you explicitly asked me to remember it as the answer to the initial question, the correct answer to **“What’s my name?”** is **Venkat**.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the same config, ask another question...\n",
    "\n",
    "response=with_message_history.invoke(\n",
    "\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "     config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0494abb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Answer –\\u202fWhere you live:** **New\\u202fJersey**\\n\\n---\\n\\n## How I arrived at that answer (full reasoning)\\n\\n1. **Your explicit statement**  \\n   - In the message where you asked me to “remember,” you wrote:  \\n     > “Hi, My name is **Venkat**, I am a **AI Engineer** and I **live in New Jersey**.”  \\n   - This sentence directly provides the location: **New\\u202fJersey**.\\n\\n2. **Instruction to remember**  \\n   - You told me to keep that exact phrasing as the definitive answer to the *initial question* **“What do I live?”**.  \\n   - I stored the three pieces of information you gave me:  \\n     - **Name**\\u202f=\\u202fVenkat  \\n     - **Profession**\\u202f=\\u202fAI Engineer (also referred to as Chief AI Engineer)  \\n     - **Location**\\u202f=\\u202fNew\\u202fJersey  \\n\\n3. **Consistent retrieval in later interactions**  \\n   - When you later asked “What do I live?” I retrieved the stored location value, which is **New\\u202fJersey**.  \\n   - My previous reply (the one just above) already printed “New\\u202fJersey” as the answer, confirming the stored value.\\n\\n4. **No contradictory updates**  \\n   - Throughout the conversation you never supplied a different place of residence or indicated a change, so the only valid answer remains **New\\u202fJersey**.\\n\\n---\\n\\n### Summary\\n\\n- **Your name:**\\u202fVenkat  \\n- **Your role:**\\u202fAI Engineer (Chief AI Engineer)  \\n- **Your residence:**\\u202f**New\\u202fJersey, USA**\\n\\nThus, the answer to the initial question **“What do I live?”** (interpreted as “Where do I live?”) is **New\\u202fJersey**.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since above answer is correct, Using the same config, lets ask another question...\n",
    "\n",
    "response=with_message_history.invoke(\n",
    "\n",
    "    [HumanMessage(content=\"What do I live?\")],\n",
    "     config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a890d9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I’m sorry, but I don’t have any information about your name in the conversation history. Since it wasn’t provided, I can’t determine what it is. If you’d like me to address you by name, just let me know what it is!'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we change the config, will it be able to remember the chat context?\n",
    "# change the config ==> change the session_id\n",
    "# Whatever the conversation had with LLM, it save as session_id  \n",
    "\n",
    "config_1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "\n",
    "response=with_message_history.invoke(\n",
    "\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "     config=config_1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae68f695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Answer to your original question (“Hey, my name is Venkat?”):**\\n\\n- **Initial situation:** When you first asked “What’s my name?” I didn’t have any information about you, so I couldn’t answer.\\n- **New information:** You then told me, “Hey, my name is Venkat?”\\n- **Conclusion:** Based on the information you just provided, your name is **Venkat**.\\n\\nSo, yes—your name is Venkat. If there’s anything else you’d like to discuss or any way I can help you, just let me know!'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since LLM said dont know the name, we can initiate with name now \n",
    "response=with_message_history.invoke(\n",
    "\n",
    "    [HumanMessage(content=\"Hey, my name is Venkat?\")],\n",
    "     config=config_1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fff2864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Venkat.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# running again with what's my name? ...it should remember your name\n",
    "# with the help of session_id, we can able to save the chat session and interact with LLMs\n",
    "\n",
    "response=with_message_history.invoke(\n",
    "\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "     config=config_1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31a442e",
   "metadata": {},
   "source": [
    "### Prompt Templates\n",
    "##### Prompt templates help to turn the raw user info into a format that the LLM can work with. In this case,raw user input is just a message, which are passing to the LLM. First, lets add in a system message with some custom instructions (but still taking messages as input). Next,we'll add in more input besides just the messages. Till now, we were working with passing the messages as LIST."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f7f85c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fab516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MessagesPlaceHolder holds the messages instead of passing individual messages.\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder \n",
    "\n",
    "# Creating a prompt template\n",
    "# Whatever the human message, it should be in {key:value} pair, where key name should be \"messages\"...that's why we use MessagesPlaceholder\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "\n",
    "[\n",
    "    (\"system\",\"You are a helpful AI assistant. Answer all the questions to the best of your knowledge\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "]\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "# Create a chain \n",
    "chain=prompt|model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c02512a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Nice to meet you, Venkat! How can I assist you today?', additional_kwargs={'reasoning_content': '<Think>\\n\\n</Think>'}, response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 281, 'total_tokens': 357, 'completion_time': 0.174377, 'completion_tokens_details': None, 'prompt_time': 0.030798, 'prompt_tokens_details': None, 'queue_time': 0.265739, 'total_time': 0.205175}, 'model_name': 'groq/compound', 'system_fingerprint': None, 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b4710-2762-7990-b248-a531d8e33940-0', usage_metadata={'input_tokens': 281, 'output_tokens': 76, 'total_tokens': 357})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# message in key:value pair \n",
    "\n",
    "chain.invoke({\"messages\":[HumanMessage(content=\"My name is Venkat\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7dca616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0baadc9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Venkat! Nice to meet you. How can I help you today?'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new config \n",
    "config={\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "response=with_message_history.invoke(\n",
    "\n",
    "    [HumanMessage(content=\"Hello, My name is Venkat\")],\n",
    "     config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a6a321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, passing {language} with prompt template.\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "\n",
    "[\n",
    "    (\"system\",\"You are a helpful AI assistant. Answer all the questions to the best of your knowledge in {language}\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "]\n",
    "\n",
    ")\n",
    "\n",
    "chain = prompt | model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d3ef3541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='नमस्ते वेंकट! आपका स्वागत है। आपसे मिलकर खुशी हुई। यदि आपके पास कोई प्रश्न है या किसी चीज़ में मदद चाहिए, तो बेझिझक बताइए। मैं यहाँ आपकी सहायता के लिए हूँ।' additional_kwargs={'reasoning_content': 'नमस्ते वेंकट! मैं कंपाउंड हूँ, जो ग्रोक द्वारा बनाया गया एक सिस्टम हूँ। मैं आपके प्रश्नों का उत्तर देने के लिए तैयार हूँ। कृपया अपना प्रश्न पूछें।\\n\\n\\nमैं वेंकट के साथ बातचीत करने के लिए तैयार हूँ। मुझे लगता है कि वह कुछ पूछना चाहता है।\\n\\n<tool>python(print(\"नमस्ते वेंकट! मैं कैसे मदद कर सकता हूँ?\"))</tool>\\n<output>नमस्ते वेंकट! मैं कैसे मदद कर सकता हूँ?\\n</output>\\n\\n\\nअब आपकी बारी है! कृपया अपना प्रश्न पूछें।', 'executed_tools': [{'arguments': '{\"code\": \"print(\"नमस्ते वेंकट! मैं कैसे मदद कर सकता हूँ?\")\"}', 'index': 0, 'type': 'python', 'browser_results': None, 'code_results': None, 'output': 'नमस्ते वेंकट! मैं कैसे मदद कर सकता हूँ?\\n', 'search_results': {'images': None, 'results': []}}]} response_metadata={'token_usage': {'completion_tokens': 268, 'prompt_tokens': 1742, 'total_tokens': 2010, 'completion_time': 0.616608, 'completion_tokens_details': None, 'prompt_time': 0.063941, 'prompt_tokens_details': None, 'queue_time': 0.467111, 'total_time': 0.68055}, 'model_name': 'groq/compound', 'system_fingerprint': None, 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019b4730-f5d7-78b3-a80d-1b7df8ddffad-0' usage_metadata={'input_tokens': 1742, 'output_tokens': 268, 'total_tokens': 2010}\n"
     ]
    }
   ],
   "source": [
    "response=chain.invoke({\n",
    "\n",
    "    \n",
    "    \"messages\":[HumanMessage(content=\"Hi, My name is Venkat\")],\n",
    "    \"language\":\"Hindi\"\n",
    "    \n",
    "    })\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff27da3",
   "metadata": {},
   "source": [
    "### This time, because there are multiple keys in the input, we need to specify the correct key to use to save the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6d3e15dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2101528b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नमस्ते वेंकट! आपसे मिलकर बहुत खुशी हुई। यदि आपके पास कोई प्रश्न है या किसी चीज़ में मदद चाहिए, तो बेझिझक बताइए—मैं यहाँ आपकी सहायता के लिए हूँ।'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat4\"}}\n",
    "response=with_message_history.invoke(\n",
    "    {'messages':[HumanMessage(content=\"Hi, I am Venkat\")],\"language\":\"Hindi\"},\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "672d1ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=with_message_history.invoke(\n",
    "{'messages':[HumanMessage(content=\"What is my name\")],\"language\":\"Hindi\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e666e2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'आपने पहले बताया था कि आपका नाम **वेंकट** है।'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b9f979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00ef7f4b",
   "metadata": {},
   "source": [
    "### Managing the Conversation History\n",
    "##### One important concept to understand when building the chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the \"context window\" of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\n",
    "##### trim_messages  helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7a4acba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"You're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Its my pleasure', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Yes', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is Data Science?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Data science is the practice of turning raw data into insights, predictions, and decisions using statistics, programming, and domain knowledge.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage,trim_messages \n",
    "trimmer=trim_messages(\n",
    "    max_tokens=70,          # setting max no. of token for context\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,    # system messages\n",
    "    allow_partial=False,    # means no partial response \n",
    "    start_on=\"human\"        # start from the human conversation\n",
    ")\n",
    "\n",
    "# here, we are setting up the LLM context \n",
    "messages=[\n",
    "    SystemMessage(content=\"You're a good assistant\"),\n",
    "    HumanMessage(content=\"Hi, am Venkat\"),\n",
    "    AIMessage(content=\"Hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"What is 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"Thanks\"),\n",
    "    AIMessage(content=\"Its my pleasure\"),\n",
    "    HumanMessage(content=\"Having fun?\"),\n",
    "    AIMessage(content=\"Yes\"),\n",
    "    HumanMessage(content=\"What is Data Science?\"),\n",
    "    AIMessage(content=\"Data science is the practice of turning raw data into insights, predictions, and decisions using statistics, programming, and domain knowledge.\")\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "448fea2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the information we have so far, I don’t actually know your personal favorite ice‑cream flavor—\\u200byou haven’t shared any specific details about your tastes.\\u202fThe only “reasoning” I can offer right now is a general one:\\n\\n1. **What I know:**  \\n   * You asked, “What ice cream do I like?”  \\n   * I have no direct data about your past choices, dietary restrictions, or flavor preferences.\\n\\n2. **What I can infer:**  \\n   * In the absence of personal clues, the safest approach is to look at the most‑commonly‑enjoyed flavors.\\u202fThese are the flavors that many people tend to like, so there’s a reasonable chance one of them could be yours as well.\\n\\n3. **Popular ice‑cream flavors (the ones most people gravitate toward):**  \\n   - **Vanilla** – classic, versatile, often a go‑to for toppings or desserts.  \\n   - **Chocolate** – rich, indulgent, a favorite for chocolate lovers.  \\n   - **Strawberry** – fruity and slightly tart, a common “fruit” choice.  \\n   - **Cookies\\u202fand\\u202fCream** – crunchy cookie bits in a sweet cream base.  \\n   - **Mint\\u202fChocolate\\u202fChip** – refreshing mint with chocolate chunks.\\n\\n4. **Conclusion / Answer:**  \\n   *Because I don’t have any personal data about you, I can’t pinpoint a single flavor you like.*  \\n   *If you enjoy any of the above “popular” options, one of them might be your favorite. If you can share a hint—e.g., “I love chocolate” or “I’m allergic to nuts”—I can give a more precise answer.*\\n\\n**Bottom line:**\\u202fWithout additional clues, the best I can do is list the most popular flavors (vanilla, chocolate, strawberry, cookies‑and‑cream, mint chocolate chip) as likely candidates for your preference. Feel free to let me know which of those (or any other) you actually favor, and I’ll tailor my response accordingly!'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain=(\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "# assigning to the variable \n",
    "response=chain.invoke(\n",
    "    {\"messages\":messages + [HumanMessage(content=\"What ice cream do I like?\")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# check the response \n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2ab0537e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Your original Data‑Science‑related question was:**\\n\\n> **“What is Data Science?”**\\n\\n---\\n\\n### How I determined that\\n\\n1. **Conversation history** – Earlier you asked, “What is Data Science?” and I responded with a concise definition.\\n2. **Your follow‑up** – You then asked, “What Data Science question I asked?” which is essentially a request to recall the exact wording of the previous query.\\n3. **Clarification** – You later restated the request: “Remember, the initial question I asked was: What Data Science question I asked?” confirming that you want the original question identified.\\n\\n---\\n\\n### The answer (including the definition I previously gave)\\n\\n> **Question:** *What is Data Science?*  \\n> **Answer:** Data science is the practice of turning raw data into insights, predictions, and decisions using statistics, programming, and domain knowledge. It combines techniques from mathematics, computer science, and subject‑matter expertise to extract meaningful information from structured and unstructured data.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask another question\n",
    "\n",
    "response=chain.invoke(\n",
    "    {\"messages\":messages + [HumanMessage(content=\"What Data Science question I asked?\")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36805879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, wrap this in MessageHistory with a new session_id\n",
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    "\n",
    ")\n",
    "\n",
    "config={\"configurable\":{\"session_id\":\"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "300e6816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I don’t have any information about your name from our conversation so far. If you’d like me to address you by name, could you let me know what it is?'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This message history wont give the question I asked for because this is a new session_id\n",
    "response=with_message_history.invoke(\n",
    "    {\"messages\":messages + [HumanMessage(content=\"What's my name?\")],\n",
    "    \"language\":\"English\"\n",
    "\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
