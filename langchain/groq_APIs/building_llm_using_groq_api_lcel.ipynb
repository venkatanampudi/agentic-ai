{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "481e11c0",
   "metadata": {},
   "source": [
    "### Building a Simple LLM Application with LCEL (LangChain Expression Language)\n",
    "##### This application will translate text from English to another language. This is just a single LLM call using some prompting.\n",
    "##### This application gives overview of:\n",
    "* Using language models \n",
    "* Using PromptTemplates and StrOutputParsers\n",
    "* Using LangChain Expression Language (LCEL) to chain components together \n",
    "* Debugging and tracing application with LangSmith\n",
    "* Deploying application with LangServe\n",
    "\n",
    "### Why Groq?\n",
    "* Groq is the AI infrastructure company that delivers fast AI inference\n",
    "* The LPU (Large Processing Unit) Inference Engine by Groq is a hardware and software platform that delivers exceptional compute speed,\n",
    "quality, and energy efficient\n",
    "* An LPU is Groq’s custom chip designed specifically for AI inference, especially large language models (LLMs).\n",
    "* Unlike GPUs (many cores doing many things), the LPU uses a deterministic, single-core–style architecture, so every operation happens in a predictable order.\n",
    "* LPU is designed to overcome the two LLM bottlenecks: compute density and memory bandwidth. A LPU has greater compute capacity than a GPU & CPU in regards to LLMs. This reduces the amount of time per word calculated, allowing sequence of text to be generated must faster.Additionally, eliminating external memory bottlenecks enables the LPU Inference Engine to deliver orders of magnitude better performance on LLMs compared to GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d7766a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e653f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Open AI API key and Open Source Models - Llama3,Gemma2, Mistral on platform Groq \n",
    "\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # this will load all the keys from \".env\" file\n",
    "\n",
    "import openai\n",
    "openai.api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# for groq\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# installations \n",
    "# !pip install langchain_groq\n",
    "# !pip install langchain_core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53ed9904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={}, client=<groq.resources.chat.completions.Completions object at 0x00000237C86AAF00>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000237C8E41FD0>, model_name='groq/compound', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### import ChatGoq\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"groq/compound\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6da0608b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmessages = [\\n            SystemMessage(content=\"Translate the following from English to French\"), \\n            HumanMessage(content=\"Hello,What is time now?\"),\\n        ]\\n\\nresult=model.invoke(messages)\\nresult\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.messages import HumanMessage,SystemMessage\n",
    "# systemmessage is how LLM model should be behave/instruct. In this case, it behaves like a translator\n",
    "# HumanMessage send to LLM and get the response. In another way - what sentence I need to convert?\n",
    "\"\"\"\n",
    "messages = [\n",
    "            SystemMessage(content=\"Translate the following from English to French\"), \n",
    "            HumanMessage(content=\"Hello,What is time now?\"),\n",
    "        ]\n",
    "\n",
    "result=model.invoke(messages)\n",
    "result\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97d299c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='**¿Cómo estás?**', additional_kwargs={'reasoning_content': '<Think>\\n\\n</Think>'}, response_metadata={'token_usage': {'completion_tokens': 61, 'prompt_tokens': 259, 'total_tokens': 320, 'completion_time': 0.131116, 'completion_tokens_details': None, 'prompt_time': 0.010973, 'prompt_tokens_details': None, 'queue_time': 0.477865, 'total_time': 0.142088}, 'model_name': 'groq/compound', 'system_fingerprint': None, 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b4178-e942-7933-90d7-12dba6721f77-0', usage_metadata={'input_tokens': 259, 'output_tokens': 61, 'total_tokens': 320})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing from English to Spanish\n",
    "\n",
    "messages = [\n",
    "            SystemMessage(content=\"Translate the following from English to Spanish\"), \n",
    "            HumanMessage(content=\"How are you?\"),\n",
    "        ]\n",
    "\n",
    "result=model.invoke(messages)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874c0bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**¿Cómo estás?**'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# StrOutputParser display custom or AI message and convert into String format (discard all other details)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser=StrOutputParser()\n",
    "parser.invoke(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a4abe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿Cómo estás?'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### using LCEL - we can chain (one after the other) the components\n",
    "### chain=model|parser ==> when invoke() method executed first messages will pass to the model and get the result and that result\n",
    "### pass to parser to convert StrOutputParser\n",
    "\n",
    "chain=model|parser\n",
    "chain.invoke(messages)  # messages = SystemMessage,HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2275447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prompt Templates - instead of passing many messages individually, create a prompt template to make it generic\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "generic_template=\"Translate the following into {language}:\"\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [(\"system\",generic_template),(\"user\",\"{text}\")]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a6721ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=prompt.invoke({\"language\":\"Spanish\",\"text\":\"Hello\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b6d35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following into Spanish:', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_messages() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a616630b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'నా పేరు వెంకట్.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chaining togethter components with LCEL\n",
    "\n",
    "chain=prompt|model|parser\n",
    "chain.invoke({\"language\":\"Telugu\",\"text\":\"My name is Venkat\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc04ed80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bonjour'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"language\":\"French\",\"text\":\"Hello\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e492fb48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
